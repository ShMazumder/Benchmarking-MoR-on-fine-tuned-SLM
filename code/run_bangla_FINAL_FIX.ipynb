{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Bangla Training - FINAL FIX (Aggressive Optimizations)\n",
                "\n",
                "This notebook implements **all aggressive fixes** to solve the Bangla convergence failure:\n",
                "\n",
                "## What Was Wrong:\n",
                "- ‚ùå Loss stuck at 8.14 (no learning)\n",
                "- ‚ùå Accuracy flat at 3.67% (random guessing)\n",
                "- ‚ùå Even with 16K vocab + 10 epochs\n",
                "\n",
                "## Fixes Implemented:\n",
                "1. ‚úÖ **Curated Corpus**: ai4bharat/IndicNLPSuite (professional-grade)\n",
                "2. ‚úÖ **Lower LR**: 1e-4 (vs 3e-4)\n",
                "3. ‚úÖ **LR Warmup**: 10% of training\n",
                "4. ‚úÖ **Cosine Annealing**: Smooth LR decay\n",
                "5. ‚úÖ **Gradient Clipping**: max_norm=1.0\n",
                "6. ‚úÖ **Weight Decay**: 0.01 regularization\n",
                "7. ‚úÖ **Smaller Batch**: 64 (vs 128)\n",
                "\n",
                "## Expected Results:\n",
                "- **Loss**: 8.14 ‚Üí **<6.0** ‚úÖ\n",
                "- **Accuracy**: 3.67% ‚Üí **15-25%** ‚úÖ\n",
                "\n",
                "**Compatible with:** Kaggle P100, Google Colab, Local GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Environment Setup\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# Detect environment\n",
                "IN_COLAB = False\n",
                "IN_KAGGLE = False\n",
                "\n",
                "try:\n",
                "    import google.colab\n",
                "    IN_COLAB = True\n",
                "    print(\"Environment: Google Colab\")\n",
                "except ImportError:\n",
                "    if os.path.exists('/kaggle'):\n",
                "        IN_KAGGLE = True\n",
                "        print(\"Environment: Kaggle\")\n",
                "    else:\n",
                "        print(\"Environment: Local PC\")\n",
                "\n",
                "# Setup repository\n",
                "if os.path.exists('train_amp_v2.py'):\n",
                "    print(f\"Already in code directory: {os.getcwd()}\")\n",
                "else:\n",
                "    REPO_URL = \"https://github.com/ShMazumder/Benchmarking-MoR-on-fine-tuned-SLM.git\"\n",
                "    REPO_DIR = \"Benchmarking-MoR-on-fine-tuned-SLM\"\n",
                "    \n",
                "    if not os.path.exists(REPO_DIR):\n",
                "        print(f\"Cloning repository...\")\n",
                "        !git clone {REPO_URL}\n",
                "    \n",
                "    if os.path.exists(os.path.join(REPO_DIR, 'code')):\n",
                "        os.chdir(os.path.join(REPO_DIR, 'code'))\n",
                "    elif os.path.exists('code'):\n",
                "        os.chdir('code')\n",
                "    \n",
                "    print(f\"Changed to: {os.getcwd()}\")\n",
                "\n",
                "# Install dependencies\n",
                "if IN_COLAB or IN_KAGGLE:\n",
                "    print(\"Installing dependencies...\")\n",
                "    !pip install -r requirements.txt --quiet\n",
                "    !pip install datasets sentencepiece --quiet\n",
                "    print(\"‚úì Dependencies installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "check_gpu",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Check GPU\n",
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"‚úì Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
                "else:\n",
                "    print(\"‚ö† WARNING: No GPU detected!\")\n",
                "    print(\"Training will be extremely slow.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "corpus_section",
            "metadata": {},
            "source": [
                "## Download Curated Bangla Corpus\n",
                "\n",
                "Using **ai4bharat/IndicNLPSuite** - professional-grade corpus with:\n",
                "- ‚úÖ Quality filtering\n",
                "- ‚úÖ Deduplication\n",
                "- ‚úÖ Proper cleaning\n",
                "- ‚úÖ 60%+ Bangla character requirement"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "download_corpus",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Download Curated Bangla Corpus\n",
                "from pathlib import Path\n",
                "from datasets import load_dataset\n",
                "import re\n",
                "\n",
                "def clean_bangla_text(text):\n",
                "    \"\"\"Aggressive cleaning\"\"\"\n",
                "    # Remove URLs, emails\n",
                "    text = re.sub(r'http\\S+|www\\S+|\\S+@\\S+', '', text)\n",
                "    # Remove excessive whitespace\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    # Split on Bangla sentence enders\n",
                "    text = re.sub(r'([‡•§!?])\\s*', r'\\1\\n', text)\n",
                "    \n",
                "    # Filter lines with <60% Bangla characters\n",
                "    lines = []\n",
                "    for line in text.split('\\n'):\n",
                "        line = line.strip()\n",
                "        if len(line) < 20:\n",
                "            continue\n",
                "        bangla_chars = len(re.findall(r'[\\u0980-\\u09FF]', line))\n",
                "        total_chars = len(re.sub(r'\\s', '', line))\n",
                "        if total_chars > 0 and bangla_chars / total_chars > 0.6:\n",
                "            lines.append(line)\n",
                "    \n",
                "    # Deduplicate\n",
                "    lines = list(dict.fromkeys(lines))\n",
                "    return '\\n'.join(lines).strip()\n",
                "\n",
                "BANGLA_PATH = Path('data/bangla/bangla_curated.txt')\n",
                "\n",
                "if not BANGLA_PATH.exists():\n",
                "    print(\"Downloading CURATED Bangla corpus...\")\n",
                "    BANGLA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    try:\n",
                "        print(\"Source: ai4bharat/IndicNLPSuite (high-quality)\")\n",
                "        dataset = load_dataset(\n",
                "            \"ai4bharat/IndicNLPSuite\",\n",
                "            \"bn\",\n",
                "            split=\"train\",\n",
                "            streaming=True\n",
                "        )\n",
                "    except:\n",
                "        print(\"Fallback: Wikipedia (cleaned)\")\n",
                "        dataset = load_dataset(\n",
                "            'wikimedia/wikipedia',\n",
                "            '20231101.bn',\n",
                "            split='train',\n",
                "            streaming=True\n",
                "        )\n",
                "    \n",
                "    target_size = 20 * 1024 * 1024  # 20MB\n",
                "    current_size = 0\n",
                "    texts = []\n",
                "    \n",
                "    for i, article in enumerate(dataset):\n",
                "        text = article.get('text') or article.get('content', '')\n",
                "        text = clean_bangla_text(text)\n",
                "        \n",
                "        if len(text) < 100:\n",
                "            continue\n",
                "        \n",
                "        texts.append(text)\n",
                "        current_size += len(text.encode('utf-8'))\n",
                "        \n",
                "        if i % 100 == 0:\n",
                "            print(f\"  {current_size / 1024 / 1024:.2f}MB ({len(texts)} articles)\")\n",
                "        \n",
                "        if current_size >= target_size:\n",
                "            break\n",
                "    \n",
                "    with open(BANGLA_PATH, 'w', encoding='utf-8') as f:\n",
                "        f.write('\\n\\n'.join(texts))\n",
                "    \n",
                "    print(f\"‚úì Saved {current_size / 1024 / 1024:.2f}MB to {BANGLA_PATH}\")\n",
                "    print(f\"‚úì Articles: {len(texts)}\")\n",
                "else:\n",
                "    print(f\"‚úì Curated corpus found: {BANGLA_PATH}\")\n",
                "\n",
                "# Show sample\n",
                "with open(BANGLA_PATH, 'r', encoding='utf-8') as f:\n",
                "    sample = f.read(300)\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"SAMPLE:\")\n",
                "print(\"=\"*50)\n",
                "print(sample + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config_section",
            "metadata": {},
            "source": [
                "## Apply Aggressive Configuration\n",
                "\n",
                "### Key Changes:\n",
                "1. **Batch Size**: 128 ‚Üí **64** (better gradient estimates)\n",
                "2. **Learning Rate**: 3e-4 ‚Üí **1e-4** (prevents overshooting)\n",
                "3. **Scheduler**: None ‚Üí **Cosine with Warmup**\n",
                "4. **Gradient Clipping**: None ‚Üí **1.0**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "apply_config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Apply Aggressive Config\n",
                "config_path = 'config.py'\n",
                "\n",
                "if os.path.exists(config_path):\n",
                "    with open(config_path, 'r') as f:\n",
                "        content = f.read()\n",
                "    \n",
                "    # Reduce batch size for better gradients\n",
                "    if 'batch_size = 128' in content:\n",
                "        content = content.replace('batch_size = 128', 'batch_size = 64')\n",
                "        print(\"‚úì Batch size: 128 ‚Üí 64\")\n",
                "    elif 'batch_size = 64' in content:\n",
                "        print(\"‚úì Batch size already 64\")\n",
                "    \n",
                "    # Lower learning rate\n",
                "    if 'learning_rate = 3e-4' in content:\n",
                "        content = content.replace('learning_rate = 3e-4', 'learning_rate = 1e-4')\n",
                "        print(\"‚úì Learning rate: 3e-4 ‚Üí 1e-4\")\n",
                "    elif 'learning_rate = 1e-4' in content:\n",
                "        print(\"‚úì Learning rate already 1e-4\")\n",
                "    \n",
                "    with open(config_path, 'w') as f:\n",
                "        f.write(content)\n",
                "    \n",
                "    print(\"\\n‚úì Configuration optimized!\")\n",
                "else:\n",
                "    print(\"‚ö† config.py not found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_section",
            "metadata": {},
            "source": [
                "## Run Training with Aggressive Optimizations\n",
                "\n",
                "Using `train_amp_v2.py` which includes:\n",
                "- ‚úÖ LR warmup (10% of training)\n",
                "- ‚úÖ Cosine annealing\n",
                "- ‚úÖ Gradient clipping (max_norm=1.0)\n",
                "- ‚úÖ Weight decay (0.01)\n",
                "- ‚úÖ Better optimizer settings\n",
                "\n",
                "### What to Watch:\n",
                "**‚úÖ Good Signs:**\n",
                "- Loss decreases: 8.1 ‚Üí 7.5 ‚Üí 7.0 ‚Üí 6.5\n",
                "- Accuracy increases: 3% ‚Üí 5% ‚Üí 10% ‚Üí 15%\n",
                "- LR decreases smoothly\n",
                "\n",
                "**‚ùå Bad Signs:**\n",
                "- Loss stays flat after 3 epochs\n",
                "- Accuracy stuck at 3%"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_baseline_6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Train Baseline N=6 (Should converge well)\n",
                "print(\"=\"*70)\n",
                "print(\"TRAINING: Baseline N=6 (Shallow - Should Work)\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "!python train_amp_v2.py \\\n",
                "    --dataset bangla \\\n",
                "    --experiment baseline_6 \\\n",
                "    --tokenization subword \\\n",
                "    --subword_vocab_size 16000 \\\n",
                "    --epochs 10 \\\n",
                "    --device cuda \\\n",
                "    --amp\n",
                "\n",
                "print(\"\\n‚úì Baseline N=6 completed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_baseline_12",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Train Baseline N=12 (Deep - Testing fixes)\n",
                "print(\"=\"*70)\n",
                "print(\"TRAINING: Baseline N=12 (Deep - With Aggressive Fixes)\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "!python train_amp_v2.py \\\n",
                "    --dataset bangla \\\n",
                "    --experiment baseline_12 \\\n",
                "    --tokenization subword \\\n",
                "    --subword_vocab_size 16000 \\\n",
                "    --epochs 10 \\\n",
                "    --device cuda \\\n",
                "    --amp\n",
                "\n",
                "print(\"\\n‚úì Baseline N=12 completed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_mor",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Train MoR Models\n",
                "experiments = [\n",
                "    (\"mor_exp1\", \"MoR Exp1 (Efficiency)\"),\n",
                "    (\"mor_exp2\", \"MoR Exp2 (Equal Cost)\")\n",
                "]\n",
                "\n",
                "for exp_name, exp_desc in experiments:\n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(f\"TRAINING: {exp_desc}\")\n",
                "    print(\"=\"*70)\n",
                "    \n",
                "    !python train_amp_v2.py \\\n",
                "        --dataset bangla \\\n",
                "        --experiment {exp_name} \\\n",
                "        --tokenization subword \\\n",
                "        --subword_vocab_size 16000 \\\n",
                "        --epochs 10 \\\n",
                "        --device cuda \\\n",
                "        --amp\n",
                "    \n",
                "    print(f\"\\n‚úì {exp_desc} completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "results_section",
            "metadata": {},
            "source": [
                "## Analyze Results\n",
                "\n",
                "Compare before vs after aggressive fixes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Analyze Results\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "results_dir = Path('results')\n",
                "\n",
                "# Load results\n",
                "results = {}\n",
                "for exp in ['baseline_6', 'baseline_12', 'mor_exp1', 'mor_exp2']:\n",
                "    result_file = results_dir / f'bangla_{exp}.json'\n",
                "    if result_file.exists():\n",
                "        with open(result_file) as f:\n",
                "            results[exp] = json.load(f)\n",
                "\n",
                "# Create comparison table\n",
                "df = pd.DataFrame(results).T\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"RESULTS SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(df[['test_accuracy', 'test_loss', 'training_time']])\n",
                "\n",
                "# Plot training curves\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "for exp in results.keys():\n",
                "    history_file = results_dir / f'bangla_{exp}_history.json'\n",
                "    if history_file.exists():\n",
                "        with open(history_file) as f:\n",
                "            history = json.load(f)\n",
                "        \n",
                "        epochs = [h['epoch'] for h in history]\n",
                "        loss = [h['loss'] for h in history]\n",
                "        acc = [h['acc'] for h in history]\n",
                "        \n",
                "        axes[0].plot(epochs, loss, marker='o', label=exp, linewidth=2)\n",
                "        axes[1].plot(epochs, acc, marker='s', label=exp, linewidth=2)\n",
                "\n",
                "axes[0].set_title('Training Loss', fontweight='bold', fontsize=13)\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].set_title('Training Accuracy', fontweight='bold', fontsize=13)\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy (%)')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('bangla_aggressive_fixes_results.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úì Results plotted and saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison_section",
            "metadata": {},
            "source": [
                "## Before vs After Comparison\n",
                "\n",
                "### Original (Failed):\n",
                "- Vocab: 4K, Epochs: 2, LR: 1e-3 (fixed)\n",
                "- **Baseline N=12**: 3.09% accuracy, Loss stuck at 7.2\n",
                "- **Baseline N=6**: 25.77% accuracy\n",
                "\n",
                "### Improved (Expected):\n",
                "- Vocab: 16K, Epochs: 10, LR: 1e-4 (scheduled)\n",
                "- **Baseline N=12**: **15-25%** accuracy, Loss **<6.0**\n",
                "- **Baseline N=6**: **30-35%** accuracy\n",
                "\n",
                "### Success Criteria:\n",
                "‚úÖ **Baseline N=6** > 30% accuracy\n",
                "‚úÖ **Baseline N=12** > 15% accuracy\n",
                "‚úÖ **Loss** decreases steadily\n",
                "‚úÖ **MoR** matches or exceeds baselines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "final_summary",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Final Summary\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"AGGRESSIVE FIXES SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(\"\\n‚úÖ IMPLEMENTED:\")\n",
                "print(\"  1. Curated corpus (ai4bharat/IndicNLPSuite)\")\n",
                "print(\"  2. 16K vocabulary (vs 4K)\")\n",
                "print(\"  3. 10 epochs (vs 2)\")\n",
                "print(\"  4. Lower LR: 1e-4 (vs 3e-4)\")\n",
                "print(\"  5. LR warmup + cosine annealing\")\n",
                "print(\"  6. Gradient clipping (max_norm=1.0)\")\n",
                "print(\"  7. Weight decay (0.01)\")\n",
                "print(\"  8. Smaller batch size (64 vs 128)\")\n",
                "\n",
                "if results:\n",
                "    baseline_6_acc = results.get('baseline_6', {}).get('test_accuracy', 0)\n",
                "    baseline_12_acc = results.get('baseline_12', {}).get('test_accuracy', 0)\n",
                "    \n",
                "    print(\"\\nüìä RESULTS:\")\n",
                "    print(f\"  Baseline N=6:  {baseline_6_acc:.2f}% (Target: >30%)\")\n",
                "    print(f\"  Baseline N=12: {baseline_12_acc:.2f}% (Target: >15%)\")\n",
                "    \n",
                "    if baseline_6_acc > 30 and baseline_12_acc > 15:\n",
                "        print(\"\\nüéâ SUCCESS! Aggressive fixes worked!\")\n",
                "    elif baseline_6_acc > 30:\n",
                "        print(\"\\n‚ö† Partial success: N=6 works, N=12 still struggles\")\n",
                "        print(\"   This is scientifically valid - shows deep models need more data\")\n",
                "    else:\n",
                "        print(\"\\n‚ùå Still failing - may need even more aggressive fixes\")\n",
                "        print(\"   Consider: LR=5e-5, batch=32, or different corpus\")\n",
                "else:\n",
                "    print(\"\\n‚è≥ Training in progress or results not found\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
