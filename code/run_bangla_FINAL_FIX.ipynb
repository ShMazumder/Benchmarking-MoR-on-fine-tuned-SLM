{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b63d2c72",
      "metadata": {},
      "source": [
        "# Bangla Training - FINAL FIX\n",
        "\n",
        "## Fixes:\n",
        "1. ✅ Curated corpus (20MB Wikipedia)\n",
        "2. ✅ 16K vocab, 10 epochs\n",
        "3. ✅ LR 1e-4 + warmup + cosine\n",
        "4. ✅ Gradient clipping + weight decay\n",
        "5. ✅ Batch 64\n",
        "\n",
        "**Target**: Loss <6.0, Acc >15%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0c375e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if os.path.exists('/content'):\n",
        "    print(\"Environment: Colab\")\n",
        "elif os.path.exists('/kaggle'):\n",
        "    print(\"Environment: Kaggle\")\n",
        "else:\n",
        "    print(\"Environment: Local\")\n",
        "\n",
        "if not os.path.exists('train_amp_v2.py'):\n",
        "    !git clone https://github.com/ShMazumder/Benchmarking-MoR-on-fine-tuned-SLM.git\n",
        "    os.chdir('Benchmarking-MoR-on-fine-tuned-SLM/code')\n",
        "    print(f\"Changed to: {os.getcwd()}\")\n",
        "\n",
        "!pip install -q datasets sentencepiece\n",
        "print(\"✓ Ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d93c34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. GPU Check\n",
        "import torch\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d666526",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Download Corpus (Wikipedia - more reliable)\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "def clean_bangla(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'([।!?])\\s*', r'\\1\\n', text)\n",
        "    lines = [l.strip() for l in text.split('\\n') if len(l.strip()) > 20]\n",
        "    return '\\n'.join(list(dict.fromkeys(lines)))\n",
        "\n",
        "corpus_path = Path('data/bangla/bangla_slm.txt')\n",
        "corpus_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not corpus_path.exists():\n",
        "    print(\"Downloading Wikipedia Bangla...\")\n",
        "    dataset = load_dataset('wikimedia/wikipedia', '20231101.bn', split='train')\n",
        "    \n",
        "    texts = []\n",
        "    size = 0\n",
        "    target = 20 * 1024 * 1024\n",
        "    \n",
        "    for i, article in enumerate(dataset):\n",
        "        text = clean_bangla(article['text'])\n",
        "        if len(text) > 100:\n",
        "            texts.append(text)\n",
        "            size += len(text.encode('utf-8'))\n",
        "            if i % 100 == 0:\n",
        "                print(f\"  {size/1024/1024:.1f}MB\")\n",
        "            if size >= target:\n",
        "                break\n",
        "    \n",
        "    corpus_path.write_text('\\n\\n'.join(texts), encoding='utf-8')\n",
        "    print(f\"✓ Saved {size/1024/1024:.1f}MB\")\n",
        "else:\n",
        "    print(\"✓ Corpus exists\")\n",
        "\n",
        "print(f\"Sample: {corpus_path.read_text(encoding='utf-8')[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c50dd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Config\n",
        "with open('config.py', 'r') as f:\n",
        "    cfg = f.read()\n",
        "cfg = cfg.replace('batch_size = 128', 'batch_size = 64')\n",
        "cfg = cfg.replace('learning_rate = 3e-4', 'learning_rate = 1e-4')\n",
        "with open('config.py', 'w') as f:\n",
        "    f.write(cfg)\n",
        "print(\"✓ Config: batch=64, lr=1e-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ede8c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Train All\n",
        "for exp in ['baseline_6', 'baseline_12', 'mor_exp1', 'mor_exp2']:\n",
        "    print(f\"\\n{'='*70}\\nTRAINING: {exp}\\n{'='*70}\")\n",
        "    !python train_amp_v2.py --dataset bangla --experiment {exp} --tokenization subword --subword_vocab_size 16000 --epochs 10 --device cuda --amp\n",
        "    print(f\"✓ {exp} done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a86a445",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Results\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results = {}\n",
        "for exp in ['baseline_6', 'baseline_12', 'mor_exp1', 'mor_exp2']:\n",
        "    f = Path(f'results/bangla_{exp}.json')\n",
        "    if f.exists():\n",
        "        results[exp] = json.loads(f.read_text())\n",
        "\n",
        "print(\"\\nRESULTS:\")\n",
        "for name, data in results.items():\n",
        "    acc = data.get('test_accuracy', 0)\n",
        "    loss = data.get('test_loss', 0)\n",
        "    print(f\"{name:15s}: {acc:5.2f}% acc, {loss:.4f} loss\")\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "for exp in results:\n",
        "    h = Path(f'results/bangla_{exp}_history.json')\n",
        "    if h.exists():\n",
        "        hist = json.loads(h.read_text())\n",
        "        epochs = [x['epoch'] for x in hist]\n",
        "        ax1.plot(epochs, [x['loss'] for x in hist], 'o-', label=exp)\n",
        "        ax2.plot(epochs, [x['acc'] for x in hist], 's-', label=exp)\n",
        "\n",
        "ax1.set(xlabel='Epoch', ylabel='Loss', title='Training Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "ax2.set(xlabel='Epoch', ylabel='Accuracy (%)', title='Training Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Success check\n",
        "b6 = results.get('baseline_6', {}).get('test_accuracy', 0)\n",
        "b12 = results.get('baseline_12', {}).get('test_accuracy', 0)\n",
        "print(f\"\\nN=6: {b6:.1f}% (target >30%)\")\n",
        "print(f\"N=12: {b12:.1f}% (target >15%)\")\n",
        "if b6 > 30 and b12 > 15:\n",
        "    print(\"✅ SUCCESS!\")\n",
        "elif b6 > 30:\n",
        "    print(\"⚠️ Partial (N=6 works, N=12 struggles)\")\n",
        "else:\n",
        "    print(\"❌ Failed - need more fixes\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
