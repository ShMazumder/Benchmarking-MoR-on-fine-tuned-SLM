{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcbf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Clone repo (useful on fresh Kaggle session) and install deps\n",
    "!git clone https://github.com/ShMazumder/Benchmarking-MoR-on-fine-tuned-SLM.git || true\n",
    "%cd Benchmarking-MoR-on-fine-tuned-SLM/code\n",
    "# Install requirements (Kaggle may already have torch; this will install others)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12bca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) (Optional) Reduce epochs in config.py for quicker testing\n",
    "from pathlib import Path\n",
    "cfg_path = Path('config.py')\n",
    "if cfg_path.exists():\n",
    "    cfg = cfg_path.read_text()\n",
    "    cfg = cfg.replace('epochs_baseline = 30','epochs_baseline = 3')\n",
    "    cfg = cfg.replace('epochs_mor_exp1 = 30','epochs_mor_exp1 = 3')\n",
    "    cfg = cfg.replace('epochs_mor_exp2 = 50','epochs_mor_exp2 = 5')\n",
    "    cfg_path.write_text(cfg)\n",
    "    print('Updated config.py to smaller epoch counts for quick tests')\n",
    "else:\n",
    "    print('config.py not found; skipping epoch reduction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa4217",
   "metadata": {},
   "source": [
    "## 3) Run Tiny Shakespeare experiments\n",
    "The commands below mirror `run_all_experiments.sh` for the Tiny Shakespeare dataset. Run cells one at a time so you can monitor logs and abort if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Training Baseline N=12\n",
    "!echo '--- Training Baseline N=12 (shakespeare) ---'\n",
    "!python train.py --dataset shakespeare --experiment baseline_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11171150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Training MoR Exp1 (N=12)\n",
    "!echo '--- Training MoR Exp1 (shakespeare) ---'\n",
    "!python train.py --dataset shakespeare --experiment mor_exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Training Baseline N=6\n",
    "!echo '--- Training Baseline N=6 (shakespeare) ---'\n",
    "!python train.py --dataset shakespeare --experiment baseline_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efcbac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Training MoR Exp2 (N=12, Eâ‰ˆ6)\n",
    "!echo '--- Training MoR Exp2 (shakespeare) ---'\n",
    "!python train.py --dataset shakespeare --experiment mor_exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44840942",
   "metadata": {},
   "source": [
    "## 4) Run WikiText-2 experiments\n",
    "Proceed similarly for the wikitext dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Training Baseline N=12 (wikitext)\n",
    "!echo '--- Training Baseline N=12 (wikitext) ---'\n",
    "!python train.py --dataset wikitext --experiment baseline_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702312e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Training MoR Exp1 (wikitext)\n",
    "!echo '--- Training MoR Exp1 (wikitext) ---'\n",
    "!python train.py --dataset wikitext --experiment mor_exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Training Baseline N=6 (wikitext)\n",
    "!echo '--- Training Baseline N=6 (wikitext) ---'\n",
    "!python train.py --dataset wikitext --experiment baseline_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44121f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Training MoR Exp2 (wikitext)\n",
    "!echo '--- Training MoR Exp2 (wikitext) ---'\n",
    "!python train.py --dataset wikitext --experiment mor_exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b76617",
   "metadata": {},
   "source": [
    "## 5) Aggregate results and plot summaries\n",
    "This cell searches the `results/` directory for result JSONs and history files, prints a summary table, and (if history files exist) plots training loss and accuracy for each history file found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = Path('results')\n",
    "if not p.exists():\n",
    "    print('No results/ directory found. Run the experiment cells first.')\n",
    "else:\n",
    "    rows = []\n",
    "    for j in sorted(p.glob('*.json')):\n",
    "        try:\n",
    "            data = json.load(open(j))\n",
    "            rows.append({\n",
    "                'file': j.name,\n",
    "                'experiment': data.get('experiment'),\n",
    "                'model_type': data.get('model_type'),\n",
    "                'accuracy': data.get('accuracy'),\n",
    "                'test_accuracy': data.get('test_accuracy')\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print('Could not read', j, e)\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        display(df.sort_values(['model_type','experiment']))\n",
    "    else:\n",
    "        print('No result JSON files parsed.')\n",
    "\n",
    "    # Plot each history file (if present)\n",
    "    hist_files = list(p.glob('*_history.json'))\n",
    "    if not hist_files:\n",
    "        print('No history JSON files found (*_history.json).')\n",
    "    else:\n",
    "        for hf in hist_files:\n",
    "            try:\n",
    "                hist = json.load(open(hf))\n",
    "                epochs = [h['epoch'] for h in hist]\n",
    "                loss = [h.get('loss') for h in hist]\n",
    "                acc = [h.get('acc') for h in hist]\n",
    "                fig, ax1 = plt.subplots()\n",
    "                if any(v is not None for v in loss):\n",
    "                    ax1.plot(epochs, loss, '-o', color='tab:red', label='train loss')\n",
    "                    ax1.set_ylabel('loss', color='tab:red')\n",
    "                ax2 = ax1.twinx()\n",
    "                if any(v is not None for v in acc):\n",
    "                    ax2.plot(epochs, acc, '-s', color='tab:blue', label='train acc')\n",
    "                    ax2.set_ylabel('accuracy (%)', color='tab:blue')\n",
    "                plt.title(f'Training metrics from {hf.name}')\n",
    "                fig.tight_layout()\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print('Could not plot', hf, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872043e",
   "metadata": {},
   "source": [
    "---\n",
    "Notes:\n",
    "- Running all these experiments sequentially can be time-consuming; consider running a subset or using the optional epoch reduction cell above for quick verification.\n",
    "- To run AMP variants, replace `train.py` calls with `train_amp.py --amp` if you have `train_amp.py` available and want mixed-precision.\n",
    "- If you want me to execute this notebook here (on this machine) or adapt it for Kaggle (with specific runtime config), tell me which environment to use."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
