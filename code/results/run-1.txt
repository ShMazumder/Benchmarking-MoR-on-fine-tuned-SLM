# clone and install deps (run in a code cell)
!git clone https://github.com/ShMazumder/Benchmarking-MoR-on-fine-tuned-SLM.git
%cd Benchmarking-MoR-on-fine-tuned-SLM/code


%%bash
# optionally check preinstalled torch
python - <<'PY'
import torch, sys
print('torch', torch.__version__, 'cuda', torch.cuda.is_available())
PY

torch 2.6.0+cu124 cuda True


# install remaining deps
!pip install -r requirements.txt




# run with GPU
!python train.py --dataset shakespeare --experiment baseline_6 --device cuda

Loading shakespeare dataset...
Vocabulary size: 65

Baseline Transformer (N=6) Information:
  Total parameters: 7,923,777
  Model size: 30.23 MB (FP32)

Training Baseline_N6...
Epoch 1/30: 100%|█| 15685/15685 [22:30<00:00, 11.61it/s, loss=3.3189, acc=15.74%
Epoch 1: Loss=3.3119, Acc=15.25%
Epoch 2/30: 100%|█| 15685/15685 [22:33<00:00, 11.59it/s, loss=3.2901, acc=15.18%
Epoch 2: Loss=3.3094, Acc=15.27%
Epoch 3/30: 100%|█| 15685/15685 [22:22<00:00, 11.68it/s, loss=3.3311, acc=15.40%
Epoch 3: Loss=3.3093, Acc=15.27%
Epoch 4/30: 100%|█| 15685/15685 [22:24<00:00, 11.67it/s, loss=3.2614, acc=16.85%
Epoch 4: Loss=3.3093, Acc=15.27%
Epoch 5/30: 100%|█| 15685/15685 [22:29<00:00, 11.62it/s, loss=3.3236, acc=14.73%
Epoch 5: Loss=3.3093, Acc=15.27%
Epoch 6/30: 100%|█| 15685/15685 [22:27<00:00, 11.64it/s, loss=3.3220, acc=15.29%
Epoch 6: Loss=3.3093, Acc=15.27%
Epoch 7/30: 100%|█| 15685/15685 [22:27<00:00, 11.64it/s, loss=3.2881, acc=15.51%
Epoch 7: Loss=3.3093, Acc=15.27%
Epoch 8/30: 100%|█| 15685/15685 [22:22<00:00, 11.68it/s, loss=3.2781, acc=14.96%
Epoch 8: Loss=3.3093, Acc=15.27%
Epoch 9/30: 100%|█| 15685/15685 [22:21<00:00, 11.69it/s, loss=3.2261, acc=15.74%
Epoch 9: Loss=3.3093, Acc=15.27%
Epoch 10/30: 100%|█| 15685/15685 [22:20<00:00, 11.70it/s, loss=3.2888, acc=15.51
Epoch 10: Loss=3.3093, Acc=15.27%
Epoch 11/30: 100%|█| 15685/15685 [22:18<00:00, 11.71it/s, loss=3.3154, acc=14.96
Epoch 11: Loss=3.3093, Acc=15.27%
Epoch 12/30: 100%|█| 15685/15685 [22:17<00:00, 11.73it/s, loss=3.2460, acc=14.96
Epoch 12: Loss=3.3093, Acc=15.27%
Epoch 13/30: 100%|█| 15685/15685 [22:16<00:00, 11.74it/s, loss=3.3398, acc=14.17
Epoch 13: Loss=3.3093, Acc=15.27%
Epoch 14/30: 100%|█| 15685/15685 [22:13<00:00, 11.76it/s, loss=3.3033, acc=14.84
Epoch 14: Loss=3.3093, Acc=15.27%
Epoch 15/30: 100%|█| 15685/15685 [22:11<00:00, 11.78it/s, loss=3.2921, acc=16.29
Epoch 15: Loss=3.3093, Acc=15.27%
Epoch 16/30: 100%|█| 15685/15685 [22:08<00:00, 11.81it/s, loss=3.3604, acc=15.18
Epoch 16: Loss=3.3093, Acc=15.27%
Epoch 17/30: 100%|█| 15685/15685 [22:05<00:00, 11.83it/s, loss=3.3719, acc=14.73
Epoch 17: Loss=3.3093, Acc=15.27%
Epoch 18/30: 100%|█| 15685/15685 [22:10<00:00, 11.79it/s, loss=3.3687, acc=15.18
Epoch 18: Loss=3.3093, Acc=15.27%
Epoch 19/30: 100%|█| 15685/15685 [22:08<00:00, 11.81it/s, loss=3.2609, acc=15.18
Epoch 19: Loss=3.3093, Acc=15.27%
Epoch 20/30: 100%|█| 15685/15685 [21:47<00:00, 12.00it/s, loss=3.2785, acc=15.18
Epoch 20: Loss=3.3093, Acc=15.27%
Epoch 21/30: 100%|█| 15685/15685 [21:46<00:00, 12.01it/s, loss=3.2645, acc=15.40
Epoch 21: Loss=3.3093, Acc=15.27%
Epoch 22/30: 100%|█| 15685/15685 [21:46<00:00, 12.00it/s, loss=3.3595, acc=14.40
Epoch 22: Loss=3.3093, Acc=15.27%
Epoch 23/30: 100%|█| 15685/15685 [21:46<00:00, 12.01it/s, loss=3.2604, acc=15.51
Epoch 23: Loss=3.3093, Acc=15.27%
Epoch 24/30: 100%|█| 15685/15685 [21:45<00:00, 12.02it/s, loss=3.3359, acc=15.96
Epoch 24: Loss=3.3093, Acc=15.27%
Epoch 25/30: 100%|█| 15685/15685 [21:43<00:00, 12.04it/s, loss=3.2724, acc=15.51
Epoch 25: Loss=3.3093, Acc=15.27%
Epoch 26/30: 100%|█| 15685/15685 [21:42<00:00, 12.04it/s, loss=3.4078, acc=15.07
Epoch 26: Loss=3.3093, Acc=15.27%
Epoch 27/30: 100%|█| 15685/15685 [21:42<00:00, 12.04it/s, loss=3.3612, acc=14.51
Epoch 27: Loss=3.3093, Acc=15.27%
Epoch 28/30: 100%|█| 15685/15685 [21:38<00:00, 12.08it/s, loss=3.3195, acc=14.84
Epoch 28: Loss=3.3093, Acc=15.27%
Epoch 29/30: 100%|█| 15685/15685 [21:39<00:00, 12.07it/s, loss=3.3570, acc=15.07
Epoch 29: Loss=3.3093, Acc=15.27%
Epoch 30/30: 100%|█| 15685/15685 [21:54<00:00, 11.93it/s, loss=3.4268, acc=14.40
Epoch 30: Loss=3.3093, Acc=15.27%

Evaluating...

Results:
  Training Accuracy: 15.27%
  Test Accuracy: 14.90%
  Effective Depth: 6
  Training Time: 39803s
Results saved: results/shakespeare_baseline_6.json