#!/usr/bin/env python3
"""
Generate the fixed run_bangla_FINAL_FIX.ipynb with corpus path fix
"""
import json

notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Bangla Training - FINAL FIX (All Issues Resolved)\n",
                "\n",
                "This notebook implements **all aggressive fixes** with the corpus path issue resolved.\n",
                "\n",
                "## Fixes Implemented:\n",
                "1. ‚úÖ **Curated Corpus**: ai4bharat/IndicNLPSuite\n",
                "2. ‚úÖ **16K Vocabulary** (vs 4K)\n",
                "3. ‚úÖ **10 Epochs** (vs 2)\n",
                "4. ‚úÖ **Lower LR**: 1e-4 with warmup + cosine annealing\n",
                "5. ‚úÖ **Gradient Clipping**: max_norm=1.0\n",
                "6. ‚úÖ **Weight Decay**: 0.01\n",
                "7. ‚úÖ **Smaller Batch**: 64\n",
                "8. ‚úÖ **Path Fix**: Corpus copied to correct location\n",
                "\n",
                "**Expected**: Loss <6.0, Accuracy >15-25%"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup\n",
                "import os\n",
                "import sys\n",
                "\n",
                "IN_COLAB = False\n",
                "IN_KAGGLE = False\n",
                "\n",
                "try:\n",
                "    import google.colab\n",
                "    IN_COLAB = True\n",
                "    print(\"Environment: Google Colab\")\n",
                "except ImportError:\n",
                "    if os.path.exists('/kaggle'):\n",
                "        IN_KAGGLE = True\n",
                "        print(\"Environment: Kaggle\")\n",
                "    else:\n",
                "        print(\"Environment: Local PC\")\n",
                "\n",
                "# Setup repository\n",
                "if os.path.exists('train_amp_v2.py'):\n",
                "    print(f\"Already in code directory: {os.getcwd()}\")\n",
                "else:\n",
                "    REPO_URL = \"https://github.com/ShMazumder/Benchmarking-MoR-on-fine-tuned-SLM.git\"\n",
                "    REPO_DIR = \"Benchmarking-MoR-on-fine-tuned-SLM\"\n",
                "    \n",
                "    if not os.path.exists(REPO_DIR):\n",
                "        print(f\"Cloning repository...\")\n",
                "        !git clone {REPO_URL}\n",
                "    \n",
                "    if os.path.exists(os.path.join(REPO_DIR, 'code')):\n",
                "        os.chdir(os.path.join(REPO_DIR, 'code'))\n",
                "    elif os.path.exists('code'):\n",
                "        os.chdir('code')\n",
                "    \n",
                "    print(f\"Changed to: {os.getcwd()}\")\n",
                "\n",
                "# Install dependencies\n",
                "if IN_COLAB or IN_KAGGLE:\n",
                "    print(\"Installing dependencies...\")\n",
                "    !pip install -r requirements.txt --quiet\n",
                "    !pip install datasets sentencepiece --quiet\n",
                "    print(\"‚úì Dependencies installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "id": "check_gpu",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Check GPU\n",
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"‚úì Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
                "else:\n",
                "    print(\"‚ö† WARNING: No GPU detected!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "id": "download_corpus",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Download Curated Bangla Corpus\n",
                "from pathlib import Path\n",
                "from datasets import load_dataset\n",
                "import re\n",
                "import shutil\n",
                "\n",
                "def clean_bangla_text(text):\n",
                "    text = re.sub(r'http\\\\S+|www\\\\S+|\\\\S+@\\\\S+', '', text)\n",
                "    text = re.sub(r'\\\\s+', ' ', text)\n",
                "    text = re.sub(r'([‡•§!?])\\\\s*', r'\\\\1\\\\n', text)\n",
                "    \n",
                "    lines = []\n",
                "    for line in text.split('\\\\n'):\n",
                "        line = line.strip()\n",
                "        if len(line) < 20:\n",
                "            continue\n",
                "        bangla_chars = len(re.findall(r'[\\\\u0980-\\\\u09FF]', line))\n",
                "        total_chars = len(re.sub(r'\\\\s', '', line))\n",
                "        if total_chars > 0 and bangla_chars / total_chars > 0.6:\n",
                "            lines.append(line)\n",
                "    \n",
                "    lines = list(dict.fromkeys(lines))\n",
                "    return '\\\\n'.join(lines).strip()\n",
                "\n",
                "BANGLA_PATH = Path('data/bangla/bangla_curated.txt')\n",
                "DEFAULT_PATH = Path('data/bangla/bangla_slm.txt')  # Where data loader expects it\n",
                "\n",
                "if not BANGLA_PATH.exists():\n",
                "    print(\"Downloading CURATED Bangla corpus...\")\n",
                "    BANGLA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    try:\n",
                "        print(\"Source: ai4bharat/IndicNLPSuite\")\n",
                "        dataset = load_dataset(\"ai4bharat/IndicNLPSuite\", \"bn\", split=\"train\", streaming=True)\n",
                "    except:\n",
                "        print(\"Fallback: Wikipedia (cleaned)\")\n",
                "        dataset = load_dataset('wikimedia/wikipedia', '20231101.bn', split='train', streaming=True)\n",
                "    \n",
                "    target_size = 20 * 1024 * 1024\n",
                "    current_size = 0\n",
                "    texts = []\n",
                "    \n",
                "    for i, article in enumerate(dataset):\n",
                "        text = article.get('text') or article.get('content', '')\n",
                "        text = clean_bangla_text(text)\n",
                "        \n",
                "        if len(text) < 100:\n",
                "            continue\n",
                "        \n",
                "        texts.append(text)\n",
                "        current_size += len(text.encode('utf-8'))\n",
                "        \n",
                "        if i % 100 == 0:\n",
                "            print(f\"  {current_size / 1024 / 1024:.2f}MB ({len(texts)} articles)\")\n",
                "        \n",
                "        if current_size >= target_size:\n",
                "            break\n",
                "    \n",
                "    with open(BANGLA_PATH, 'w', encoding='utf-8') as f:\n",
                "        f.write('\\\\n\\\\n'.join(texts))\n",
                "    \n",
                "    print(f\"‚úì Saved {current_size / 1024 / 1024:.2f}MB to {BANGLA_PATH}\")\n",
                "else:\n",
                "    print(f\"‚úì Curated corpus found: {BANGLA_PATH}\")\n",
                "\n",
                "# CRITICAL FIX: Copy to default location\n",
                "DEFAULT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
                "if BANGLA_PATH.exists() and not DEFAULT_PATH.exists():\n",
                "    print(f\"\\\\nCopying corpus to default location...\")\n",
                "    shutil.copy(BANGLA_PATH, DEFAULT_PATH)\n",
                "    print(f\"‚úì Copied to {DEFAULT_PATH}\")\n",
                "elif DEFAULT_PATH.exists():\n",
                "    print(f\"‚úì Corpus already at default location\")\n",
                "\n",
                "# Show sample\n",
                "with open(DEFAULT_PATH, 'r', encoding='utf-8') as f:\n",
                "    sample = f.read(300)\n",
                "print(\"\\\\n\" + \"=\"*50)\n",
                "print(\"SAMPLE:\")\n",
                "print(\"=\"*50)\n",
                "print(sample + \"...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Apply Config\n",
                "config_path = 'config.py'\n",
                "\n",
                "if os.path.exists(config_path):\n",
                "    with open(config_path, 'r') as f:\n",
                "        content = f.read()\n",
                "    \n",
                "    if 'batch_size = 128' in content:\n",
                "        content = content.replace('batch_size = 128', 'batch_size = 64')\n",
                "        print(\"‚úì Batch size: 128 ‚Üí 64\")\n",
                "    \n",
                "    if 'learning_rate = 3e-4' in content:\n",
                "        content = content.replace('learning_rate = 3e-4', 'learning_rate = 1e-4')\n",
                "        print(\"‚úì Learning rate: 3e-4 ‚Üí 1e-4\")\n",
                "    \n",
                "    with open(config_path, 'w') as f:\n",
                "        f.write(content)\n",
                "    \n",
                "    print(\"\\\\n‚úì Configuration optimized!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "id": "train_all",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Run All Training\n",
                "experiments = [\n",
                "    ('baseline_6', 'Baseline N=6'),\n",
                "    ('baseline_12', 'Baseline N=12'),\n",
                "    ('mor_exp1', 'MoR Exp1'),\n",
                "    ('mor_exp2', 'MoR Exp2')\n",
                "]\n",
                "\n",
                "for exp_name, exp_desc in experiments:\n",
                "    print(\"\\\\n\" + \"=\"*70)\n",
                "    print(f\"TRAINING: {exp_desc}\")\n",
                "    print(\"=\"*70)\n",
                "    \n",
                "    !python train_amp_v2.py \\\\\n",
                "        --dataset bangla \\\\\n",
                "        --experiment {exp_name} \\\\\n",
                "        --tokenization subword \\\\\n",
                "        --subword_vocab_size 16000 \\\\\n",
                "        --epochs 10 \\\\\n",
                "        --device cuda \\\\\n",
                "        --amp\n",
                "    \n",
                "    print(f\"\\\\n‚úì {exp_desc} completed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "id": "results",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Analyze Results\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "\n",
                "results_dir = Path('results')\n",
                "results = {}\n",
                "\n",
                "for exp in ['baseline_6', 'baseline_12', 'mor_exp1', 'mor_exp2']:\n",
                "    result_file = results_dir / f'bangla_{exp}.json'\n",
                "    if result_file.exists():\n",
                "        with open(result_file) as f:\n",
                "            results[exp] = json.load(f)\n",
                "\n",
                "print(\"\\\\n\" + \"=\"*70)\n",
                "print(\"RESULTS SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for exp_name, data in results.items():\n",
                "    print(f\"\\\\n{exp_name.upper()}:\")\n",
                "    print(f\"  Test Accuracy: {data.get('test_accuracy', 0):.2f}%\")\n",
                "    print(f\"  Test Loss: {data.get('test_loss', 0):.4f}\")\n",
                "    print(f\"  Training Time: {data.get('training_time', 0)/60:.1f} min\")\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "for exp in results.keys():\n",
                "    history_file = results_dir / f'bangla_{exp}_history.json'\n",
                "    if history_file.exists():\n",
                "        with open(history_file) as f:\n",
                "            history = json.load(f)\n",
                "        \n",
                "        epochs = [h['epoch'] for h in history]\n",
                "        loss = [h['loss'] for h in history]\n",
                "        acc = [h['acc'] for h in history]\n",
                "        \n",
                "        axes[0].plot(epochs, loss, marker='o', label=exp, linewidth=2)\n",
                "        axes[1].plot(epochs, acc, marker='s', label=exp, linewidth=2)\n",
                "\n",
                "axes[0].set_title('Training Loss', fontweight='bold')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].set_title('Training Accuracy', fontweight='bold')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy (%)')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('bangla_results.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Success analysis\n",
                "baseline_6_acc = results.get('baseline_6', {}).get('test_accuracy', 0)\n",
                "baseline_12_acc = results.get('baseline_12', {}).get('test_accuracy', 0)\n",
                "\n",
                "print(\"\\\\n\" + \"=\"*70)\n",
                "print(f\"Baseline N=6:  {baseline_6_acc:.2f}% (Target: >30%)\")\n",
                "print(f\"Baseline N=12: {baseline_12_acc:.2f}% (Target: >15%)\")\n",
                "\n",
                "if baseline_6_acc > 30 and baseline_12_acc > 15:\n",
                "    print(\"\\\\nüéâ SUCCESS! Aggressive fixes worked!\")\n",
                "elif baseline_6_acc > 30:\n",
                "    print(\"\\\\n‚ö† Partial success: N=6 works, N=12 still struggles\")\n",
                "else:\n",
                "    print(\"\\\\n‚ùå Still failing - may need even more aggressive fixes\")\n",
                "\n",
                "print(\"=\"*70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

# Write the notebook
output_path = "run_bangla_FINAL_FIX.ipynb"
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=2)

print(f"‚úì Generated {output_path}")
print("‚úì All fixes included:")
print("  - Curated corpus download")
print("  - Automatic path fix (copies to default location)")
print("  - Aggressive training config")
print("  - Complete training loop")
print("  - Results analysis")
