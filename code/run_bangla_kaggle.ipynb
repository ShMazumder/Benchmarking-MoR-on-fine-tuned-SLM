{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Benchmarking MoR: Bangla & WikiText-2 (Universal Edition)\n",
                "\n",
                "This notebook runs the full suite of experiments for the Mixture-of-Recursion (MoR) Transformer on **Bangla** and **WikiText-2** datasets.\n",
                "\n",
                "**Compatible Environments:**\n",
                "*   **Kaggle Kernels**\n",
                "*   **Google Colab**\n",
                "*   **Local PC**\n",
                "\n",
                "**Experiments:**\n",
                "1. **Baseline N=12**: Standard Transformer with 12 layers.\n",
                "2. **MoR N=12 (Exp 1)**: MoR with 12 layers, prioritizing efficiency.\n",
                "3. **Baseline N=6**: Standard Transformer with 6 layers (comparable cost target).\n",
                "4. **MoR N=12 (Exp 2)**: MoR with 12 layers, tuned for equal cost to N=6.\n",
                "\n",
                "Includes automated setup, training, and visualization of results (Plots & Matrices)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup_repo",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup Repository & Dependencies\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "\n",
                "# Environment Detection\n",
                "IN_COLAB = False\n",
                "IN_KAGGLE = False\n",
                "\n",
                "try:\n",
                "    import google.colab\n",
                "    IN_COLAB = True\n",
                "    print(\"Detected Environment: Google Colab\")\n",
                "except ImportError:\n",
                "    if os.path.exists('/kaggle'):\n",
                "        IN_KAGGLE = True\n",
                "        print(\"Detected Environment: Kaggle\")\n",
                "    else:\n",
                "        print(\"Detected Environment: Local PC\")\n",
                "\n",
                "# Optimized Directory Logic\n",
                "# If we are already in the 'code' directory (e.g. running locally), skip cloning\n",
                "if os.path.exists('train_amp.py') and os.path.exists('config.py'):\n",
                "    print(f\"Already in code directory: {os.getcwd()}\")\n",
                "else:\n",
                "    # Need to setup\n",
                "    REPO_URL = \"https://github.com/ShMazumder/Benchmarking-MoR-on-fine-tuned-SLM.git\"\n",
                "    REPO_DIR = \"Benchmarking-MoR-on-fine-tuned-SLM\"\n",
                "\n",
                "    if not os.path.exists(REPO_DIR):\n",
                "        # Check if we are in the repo root\n",
                "        if os.path.exists('code') and os.path.exists('README.md'):\n",
                "            print(\"Already in repository root.\")\n",
                "        else:\n",
                "            print(f\"Cloning repository from {REPO_URL}...\")\n",
                "            !git clone {REPO_URL}\n",
                "    \n",
                "    # Move to code dir\n",
                "    if os.path.exists(os.path.join(REPO_DIR, 'code')):\n",
                "        os.chdir(os.path.join(REPO_DIR, 'code'))\n",
                "    elif os.path.exists('code'):\n",
                "        # If in repo root\n",
                "        os.chdir('code')\n",
                "        \n",
                "    print(f\"Changed directory to {os.getcwd()}\")\n",
                "\n",
                "# Install Requirements\n",
                "if IN_COLAB or IN_KAGGLE:\n",
                "    print(\"Installing dependencies (Cloud Environment)...\")\n",
                "    !pip install -r requirements.txt --quiet\n",
                "    !pip install seaborn matplotlib pandas scikit-learn datasets --quiet\n",
                "    print(\"Dependencies installed.\")\n",
                "else:\n",
                "    print(\"\\n[NOTICE] Local Environment detected.\")\n",
                "    print(\"Skipping automatic 'pip install' to preserve your local environment.\")\n",
                "    print(\"Please ensure you have installed usage requirements:\")\n",
                "    print(\"   pip install -r requirements.txt\")\n",
                "    print(\"   pip install seaborn matplotlib pandas scikit-learn datasets\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "check_gpu",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.2 Check GPU Status\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
                "    print(\"FP16/AMP will be enabled automatically.\")\n",
                "else:\n",
                "    print(\"WARNING: No GPU detected. Training will be extremely slow (FP32 CPU).\")\n",
                "    if IN_COLAB: print(\"Colab: Runtime -> Change runtime type -> GPU.\")\n",
                "    elif IN_KAGGLE: print(\"Kaggle: Session Options -> Accelerator -> GPU P100.\")\n",
                "    else: print(\"Local: Ensure you have NVIDIA Drivers and PyTorch with CUDA installed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "download_bangla",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.5. Download Substitute Bangla Dataset (if missing)\n",
                "# This ensures the notebook runs anywhere (Colab/Kaggle/Local) without manual file uploads.\n",
                "\n",
                "import os\n",
                "from pathlib import Path\n",
                "from datasets import load_dataset\n",
                "\n",
                "BANGLA_DATA_PATH = Path('data/bangla/bangla_slm.txt')\n",
                "\n",
                "if not BANGLA_DATA_PATH.exists():\n",
                "    print(\"Bangla dataset not found. Downloading Bangla Wikipedia subset...\")\n",
                "    BANGLA_DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    # Load Bengali Wikipedia (streaming mode to avoid full download)\n",
                "    # We aim for ~15MB of text to be slightly larger than WikiText-2 (10MB)\n",
                "    try:\n",
                "        dataset = load_dataset('wikimedia/wikipedia', '20231101.bn', split='train', streaming=True)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to load wikimedia/wikipedia: {e}. Trying fallback...\")\n",
                "        dataset = load_dataset('wikimedia/wikipedia', '20231101.bn', split='train', streaming=True)\n",
                "    \n",
                "    target_size = 15 * 1024 * 1024 # 15 MB\n",
                "    current_size = 0\n",
                "    text_accumulated = []\n",
                "    \n",
                "    print(\"downloading...\")\n",
                "    for i, article in enumerate(dataset):\n",
                "        text = article['text']\n",
                "        text_accumulated.append(text)\n",
                "        current_size += len(text.encode('utf-8'))\n",
                "        \n",
                "        if current_size >= target_size:\n",
                "            break\n",
                "        \n",
                "        if i % 100 == 0:\n",
                "            print(f\"Downloaded {current_size / 1024 / 1024:.2f} MB...\")\n",
                "            \n",
                "    with open(BANGLA_DATA_PATH, 'w', encoding='utf-8') as f:\n",
                "        f.write('\\n\\n'.join(text_accumulated))\n",
                "        \n",
                "    print(f\"Saved {current_size / 1024 / 1024:.2f} MB of Bangla text to {BANGLA_DATA_PATH}\")\n",
                "else:\n",
                "    print(f\"Bangla dataset found at {BANGLA_DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "patch_bangla",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. HOST FIX: Patch bangla.py (Fixing known issues & Pathing)\n",
                "# This patch makes the loader robust for both Kaggle input paths and local/Colab paths.\n",
                "\n",
                "bangla_py_content = '''\"\"\"Bangla SLM Dataset Loader with configurable tokenization\"\"\"\n",
                "import os\n",
                "from pathlib import Path\n",
                "import torch\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from config import Config\n",
                "\n",
                "try:\n",
                "    from data.tokenizers import load_sentencepiece, train_sentencepiece, build_word_vocab_from_text, encode_with_sp\n",
                "except Exception:\n",
                "    from tokenizers import load_sentencepiece, train_sentencepiece, build_word_vocab_from_text, encode_with_sp\n",
                "\n",
                "\n",
                "class BanglaSLMDataset(Dataset):\n",
                "    def __init__(self, seq_length=64, split='train', split_ratio=0.9, tokenization='char', tokenizer_model=None, vocab_size=None, data_file=None):\n",
                "        self.seq_length = seq_length\n",
                "        cfg = Config()\n",
                "\n",
                "        # Determine data path: use argument if provided, else use config default\n",
                "        if data_file:\n",
                "            data_path = Path(data_file)\n",
                "        else:\n",
                "            # Fallback to config \n",
                "            data_path = Path(cfg.bangla_data_file)\n",
                "\n",
                "        # Check if file exists, if not search Kaggle/Colab common paths\n",
                "        if not data_path.exists():\n",
                "            # Kaggle: check input directory for common patterns\n",
                "            search_paths = [\n",
                "                Path('/kaggle/input/bangla-slm/bangla_slm.txt'),\n",
                "                Path('/kaggle/input/bangla-dataset/bangla_slm.txt'),\n",
                "                Path('/kaggle/input/bangla.txt'),\n",
                "                # Add other potential paths here\n",
                "            ]\n",
                "            found = False\n",
                "            for kp in search_paths:\n",
                "                if kp.exists():\n",
                "                    print(f\"Found dataset at: {kp}\")\n",
                "                    data_path = kp\n",
                "                    found = True\n",
                "                    break\n",
                "\n",
                "        if data_path.exists():\n",
                "            text = data_path.read_text(encoding='utf-8')\n",
                "        else:\n",
                "             # Should not happen if preceding download cell ran correctly\n",
                "             raise FileNotFoundError(f\"Bangla dataset not found at {data_path}. Please ensure the download cell ran successfully.\")\n",
                "\n",
                "        self.tokenization = tokenization\n",
                "        if tokenization == 'char':\n",
                "            chars = sorted(list(set(text)))\n",
                "            self.vocab_size = len(chars)\n",
                "            self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
                "            self.itos = {i: ch for i, ch in enumerate(chars)}\n",
                "            data_ids = [self.stoi[ch] for ch in text]\n",
                "\n",
                "        elif tokenization == 'word':\n",
                "            stoi, itos = build_word_vocab_from_text(text)\n",
                "            self.stoi = stoi\n",
                "            self.itos = itos\n",
                "            self.vocab_size = len(stoi)\n",
                "            data_ids = [self.stoi[w] for w in text.split()]\n",
                "\n",
                "        elif tokenization == 'subword':\n",
                "            model_path = tokenizer_model or cfg.tokenizer_model_bangla\n",
                "            model_file = Path(model_path)\n",
                "            if not model_file.parent.exists():\n",
                "                 model_file.parent.mkdir(parents=True, exist_ok=True)\n",
                "            \n",
                "            if not model_file.exists():\n",
                "                if len(text) > 100:\n",
                "                     print(f\"Training SentencePiece model for Bangla at {model_file}...\")\n",
                "                     # We need to write the text to a temp file for SP training\n",
                "                     temp_train_file = 'temp_bangla_for_sp.txt'\n",
                "                     with open(temp_train_file, 'w', encoding='utf-8') as f: f.write(text)\n",
                "                     \n",
                "                     model_prefix = str(model_file.with_suffix(''))\n",
                "                     train_sentencepiece(temp_train_file, model_prefix, vocab_size or cfg.subword_vocab_size)\n",
                "                     model_file = Path(model_prefix + '.model')\n",
                "                     \n",
                "                     if os.path.exists(temp_train_file): os.remove(temp_train_file)\n",
                "                else:\n",
                "                     print(\"Text too short to train tokenizer.\")\n",
                "                     \n",
                "            if model_file.exists():\n",
                "                sp = load_sentencepiece(str(model_file))\n",
                "                self.vocab_size = sp.get_piece_size()\n",
                "                data_ids = encode_with_sp(sp, text)\n",
                "            else:\n",
                "                self.vocab_size = 100; data_ids = []\n",
                "\n",
                "        else:\n",
                "            raise ValueError(f'Unknown tokenization: {tokenization}')\n",
                "\n",
                "        data = torch.tensor(data_ids, dtype=torch.long)\n",
                "\n",
                "        split_idx = int(len(data) * split_ratio)\n",
                "        if split == 'train':\n",
                "            self.data = data[:split_idx]\n",
                "        else:\n",
                "            self.data = data[split_idx:]\n",
                "\n",
                "    def __len__(self):\n",
                "        if len(self.data) <= self.seq_length:\n",
                "            return 0\n",
                "        return len(self.data) - self.seq_length\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        x = self.data[idx:idx + self.seq_length]\n",
                "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
                "        return x, y\n",
                "\n",
                "\n",
                "def get_bangla_loaders(batch_size=64, seq_length=64, split_ratio=0.9, tokenization=None, tokenizer_model=None, vocab_size=None, data_file=None):\n",
                "    cfg = Config()\n",
                "    tokenization = tokenization or cfg.tokenization\n",
                "    tokenizer_model = tokenizer_model or (cfg.tokenizer_model_bangla if tokenization == 'subword' else None)\n",
                "    vocab_size = vocab_size or cfg.subword_vocab_size\n",
                "\n",
                "    train_dataset = BanglaSLMDataset(seq_length=seq_length, split='train', split_ratio=split_ratio,\n",
                "                                     tokenization=tokenization, tokenizer_model=tokenizer_model, vocab_size=vocab_size, data_file=data_file)\n",
                "    test_dataset = BanglaSLMDataset(seq_length=seq_length, split='test', split_ratio=split_ratio,\n",
                "                                    tokenization=tokenization, tokenizer_model=tokenizer_model, vocab_size=vocab_size, data_file=data_file)\n",
                "\n",
                "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
                "\n",
                "    return train_loader, test_loader, train_dataset.vocab_size\n",
                "'''\n",
                "\n",
                "with open('data/bangla.py', 'w') as f:\n",
                "    f.write(bangla_py_content)\n",
                "print(\"Patched data/bangla.py\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "patch_wikitext",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. HOST FIX: Patch wikitext.py to handle sentencepiece training file correctly\n",
                "wikitext_py_content = '''\"\"\"WikiText-2 Dataset Loader with configurable tokenization\"\"\"\n",
                "import torch\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from datasets import load_dataset\n",
                "from pathlib import Path\n",
                "from config import Config\n",
                "import os\n",
                "\n",
                "try:\n",
                "    from data.tokenizers import load_sentencepiece, train_sentencepiece, build_word_vocab_from_text, encode_with_sp\n",
                "except Exception:\n",
                "    from tokenizers import load_sentencepiece, train_sentencepiece, build_word_vocab_from_text, encode_with_sp\n",
                "\n",
                "\n",
                "class WikiText2Dataset(Dataset):\n",
                "    def __init__(self, seq_length=64, split='train', tokenization='char', tokenizer_model=None, vocab_size=None):\n",
                "        self.seq_length = seq_length\n",
                "        cfg = Config()\n",
                "\n",
                "        # Load WikiText-2 from HuggingFace\n",
                "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split=split)\n",
                "        text = ' '.join(dataset['text'])\n",
                "\n",
                "        self.tokenization = tokenization\n",
                "        if tokenization == 'char':\n",
                "            chars = sorted(list(set(text)))\n",
                "            self.vocab_size = len(chars)\n",
                "            self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
                "            self.itos = {i: ch for i, ch in enumerate(chars)}\n",
                "            data_ids = [self.stoi[ch] for ch in text]\n",
                "\n",
                "        elif tokenization == 'word':\n",
                "            stoi, itos = build_word_vocab_from_text(text)\n",
                "            self.stoi = stoi\n",
                "            self.itos = itos\n",
                "            self.vocab_size = len(stoi)\n",
                "            data_ids = [self.stoi[w] for w in text.split()]\n",
                "\n",
                "        elif tokenization == 'subword':\n",
                "            model_path = tokenizer_model or cfg.tokenizer_model_wikitext\n",
                "            model_file = Path(model_path)\n",
                "            if not model_file.parent.exists():\n",
                "                 model_file.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "            if not model_file.exists():\n",
                "                model_prefix = str(model_file.with_suffix(''))\n",
                "                # FIX: Ensure the raw file exists for SP training\n",
                "                raw_file = 'wikitext_raw.txt'\n",
                "                if not os.path.exists(raw_file):\n",
                "                    # Use training split for training tokenizer\n",
                "                    train_ds = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
                "                    full_text = ' '.join(train_ds['text'])\n",
                "                    with open(raw_file, 'w', encoding='utf-8') as f:\n",
                "                        f.write(full_text)\n",
                "                \n",
                "                train_sentencepiece(raw_file, model_prefix, vocab_size or cfg.subword_vocab_size)\n",
                "                model_file = Path(model_prefix + '.model')\n",
                "            sp = load_sentencepiece(str(model_file))\n",
                "            self.vocab_size = sp.get_piece_size()\n",
                "            self.stoi = None\n",
                "            self.itos = None\n",
                "            data_ids = encode_with_sp(sp, text)\n",
                "\n",
                "        else:\n",
                "            raise ValueError(f'Unknown tokenization: {tokenization}')\n",
                "\n",
                "        self.data = torch.tensor(data_ids, dtype=torch.long)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data) - self.seq_length\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        x = self.data[idx:idx + self.seq_length]\n",
                "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
                "        return x, y\n",
                "\n",
                "\n",
                "def get_wikitext_loaders(batch_size=64, seq_length=64, tokenization=None, tokenizer_model=None, vocab_size=None):\n",
                "    cfg = Config()\n",
                "    tokenization = tokenization or cfg.tokenization\n",
                "    tokenizer_model = tokenizer_model or (cfg.tokenizer_model_wikitext if tokenization == 'subword' else None)\n",
                "    vocab_size = vocab_size or cfg.subword_vocab_size\n",
                "\n",
                "    train_dataset = WikiText2Dataset(seq_length, 'train', tokenization, tokenizer_model, vocab_size)\n",
                "    val_dataset = WikiText2Dataset(seq_length, 'validation', tokenization, tokenizer_model, vocab_size)\n",
                "    test_dataset = WikiText2Dataset(seq_length, 'test', tokenization, tokenizer_model, vocab_size)\n",
                "\n",
                "    train_loader = DataLoader(\n",
                "        train_dataset,\n",
                "        batch_size=batch_size,\n",
                "        shuffle=True,\n",
                "        num_workers=2,\n",
                "        pin_memory=True\n",
                "    )\n",
                "\n",
                "    val_loader = DataLoader(\n",
                "        val_dataset,\n",
                "        batch_size=batch_size,\n",
                "        shuffle=False,\n",
                "        num_workers=2,\n",
                "        pin_memory=True\n",
                "    )\n",
                "\n",
                "    test_loader = DataLoader(\n",
                "        test_dataset,\n",
                "        batch_size=batch_size,\n",
                "        shuffle=False,\n",
                "        num_workers=2,\n",
                "        pin_memory=True\n",
                "    )\n",
                "\n",
                "    return train_loader, val_loader, test_loader, train_dataset.vocab_size\n",
                "'''\n",
                "\n",
                "with open('data/wikitext.py', 'w') as f:\n",
                "    f.write(wikitext_py_content)\n",
                "print(\"Patched data/wikitext.py\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_experiments",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Run Experiments\n",
                "import subprocess\n",
                "import torch\n",
                "\n",
                "# Define your experiment configuration\n",
                "datasets = ['bangla', 'wikitext'] \n",
                "experiments = ['baseline_12', 'mor_exp1', 'baseline_6', 'mor_exp2']\n",
                "EPOCHS = 10 # Adjust based on your time budget\n",
                "VOCAB = 4000\n",
                "\n",
                "def run_cmd(cmd):\n",
                "    print(\"Running:\", \" \".join(cmd))\n",
                "    subprocess.check_call(cmd)\n",
                "\n",
                "# Explicitly enabling AMP (Mixed Precision) flag for safety\n",
                "# Note: The script 'train_amp.py' enables AMP by default on CUDA, but we pass flag to be explicit.\n",
                "\n",
                "for datas in datasets:\n",
                "    print(f\"\\n=== Running Experiments for {datas.upper()} ===\")\n",
                "    \n",
                "    for exp in experiments:\n",
                "        cmd = [\n",
                "            sys.executable, 'train_amp.py', \n",
                "            '--dataset', datas, \n",
                "            '--experiment', exp,\n",
                "            '--tokenization', 'subword',\n",
                "            '--subword_vocab_size', str(VOCAB), \n",
                "            '--epochs', str(EPOCHS),\n",
                "            '--device', 'cuda' if torch.cuda.is_available() else 'cpu',\n",
                "            '--amp' # Request Mixed Precision\n",
                "        ]\n",
                "        try:\n",
                "            run_cmd(cmd)\n",
                "        except Exception as e:\n",
                "            print(f\"Experiment {exp} on {datas} failed: {e}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "plots_intro",
            "metadata": {},
            "source": [
                "## 5. Visualizations: Plots, Graphs, & Matrices\n",
                "We will now visualize the results:\n",
                "1. **Training Curves**: Loss and Accuracy over time.\n",
                "2. **Comparative Analysis**: Bar charts comparing Accuracy vs. Compute (Depth).\n",
                "3. **Confusion Matrix**: On the test set for the best performing model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_curves",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import glob\n",
                "import seaborn as sns\n",
                "\n",
                "# Set style\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "def plot_training_curves(dataset_name):\n",
                "    history_files = glob.glob(f'results/{dataset_name}_*_history.json')\n",
                "    if not history_files:\n",
                "        print(f\"No history files found for {dataset_name}\")\n",
                "        return\n",
                "\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
                "    \n",
                "    for hf in history_files:\n",
                "        exp_name = os.path.basename(hf).replace(f'{dataset_name}_', '').replace('_history.json', '')\n",
                "        with open(hf, 'r') as f:\n",
                "            hist = json.load(f)\n",
                "            df = pd.DataFrame(hist)\n",
                "            \n",
                "            ax1.plot(df['epoch'], df['loss'], marker='o', label=exp_name)\n",
                "            ax2.plot(df['epoch'], df['acc'], marker='s', label=exp_name)\n",
                "\n",
                "    ax1.set_title(f'{dataset_name.capitalize()}: Training Loss')\n",
                "    ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
                "    ax1.legend()\n",
                "\n",
                "    ax2.set_title(f'{dataset_name.capitalize()}: Training Accuracy')\n",
                "    ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy (%)')\n",
                "    ax2.legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "for d in datasets:\n",
                "    plot_training_curves(d)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "compare_bar",
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_comparison(dataset_name):\n",
                "    result_files = glob.glob(f'results/{dataset_name}_*.json')\n",
                "    # Filter out history files\n",
                "    result_files = [f for f in result_files if '_history' not in f]\n",
                "    \n",
                "    data = []\n",
                "    for rf in result_files:\n",
                "        with open(rf, 'r') as f:\n",
                "            res = json.load(f)\n",
                "            data.append(res)\n",
                "    \n",
                "    if not data: return\n",
                "\n",
                "    df = pd.DataFrame(data)\n",
                "    \n",
                "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
                "    \n",
                "    # Bar plot for Test Accuracy\n",
                "    sns.barplot(data=df, x='experiment', y='test_accuracy', ax=ax1, palette='viridis', alpha=0.8)\n",
                "    ax1.set_ylabel('Test Accuracy (%)')\n",
                "    ax1.set_title(f'{dataset_name.capitalize()}: Accuracy vs Effective Depth')\n",
                "    \n",
                "    # Line plot for Effective Depth on secondary axis\n",
                "    ax2 = ax1.twinx()\n",
                "    sns.lineplot(data=df, x='experiment', y='effective_depth', ax=ax2, color='red', marker='D', markersize=10, linewidth=3, sort=False)\n",
                "    ax2.set_ylabel('Effective Depth (Layers)')\n",
                "    ax2.grid(False)\n",
                "    \n",
                "    plt.show()\n",
                "\n",
                "for d in datasets:\n",
                "    plot_comparison(d)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "confusion_matrix",
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "\n",
                "# Function to generate confusion matrix for a specific model\n",
                "def plot_confusion_matrix(dataset, experiment, num_samples=1000):\n",
                "    print(f\"Generating confusion matrix for {dataset} - {experiment}...\")\n",
                "    \n",
                "    # Load config and model (This requires importing project modules)\n",
                "    from config import Config\n",
                "    from models import BaselineTransformer, MoRTransformer\n",
                "    from data import get_bangla_loaders, get_wikitext_loaders\n",
                "    \n",
                "    cfg = Config()\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    cfg.device = device\n",
                "    \n",
                "    # Get Loaders\n",
                "    if dataset == 'bangla':\n",
                "         _, test_loader, vocab_size = get_bangla_loaders(batch_size=32, tokenization='subword', vocab_size=VOCAB)\n",
                "    elif dataset == 'wikitext':\n",
                "         _, _, test_loader, vocab_size = get_wikitext_loaders(batch_size=32, tokenization='subword', vocab_size=VOCAB)\n",
                "    \n",
                "    # Initialize Model Structure\n",
                "    if 'baseline' in experiment:\n",
                "        n_layers = 12 if '12' in experiment else 6\n",
                "        model = BaselineTransformer(vocab_size, n_layers=n_layers, **vars(cfg))\n",
                "    else:\n",
                "        model = MoRTransformer(vocab_size, n_layers=12, **vars(cfg))\n",
                "    \n",
                "    # Load Weights (Find the last checkpoint)\n",
                "    ckpt_path = f\"checkpoints/{experiment}_epoch{EPOCHS}.pt\"\n",
                "    if not os.path.exists(ckpt_path):\n",
                "        print(f\"Checkpoint {ckpt_path} not found. Skipping CM.\")\n",
                "        # Try finding any checkpoint\n",
                "        avail = glob.glob(f\"checkpoints/{experiment}_epoch*.pt\")\n",
                "        if avail:\n",
                "            ckpt_path = sorted(avail)[-1]\n",
                "            print(f\"Using alternate checkpoint: {ckpt_path}\")\n",
                "        else:\n",
                "            return\n",
                "        \n",
                "    ckpt = torch.load(ckpt_path, map_location=device)\n",
                "    model.load_state_dict(ckpt['model_state'])\n",
                "    model.to(device)\n",
                "    model.eval()\n",
                "    \n",
                "    # Collect predictions\n",
                "    y_true = []\n",
                "    y_pred = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for i, (x, y) in enumerate(test_loader):\n",
                "            if i * x.size(0) > num_samples: break\n",
                "            x = x.to(device)\n",
                "            if 'mor' in experiment:\n",
                "                logits, _, _ = model(x, training=False)\n",
                "            else:\n",
                "                logits, _ = model(x)\n",
                "            \n",
                "            preds = torch.argmax(logits, dim=-1)\n",
                "            y_true.extend(y.view(-1).cpu().numpy())\n",
                "            y_pred.extend(preds.view(-1).cpu().numpy())\n",
                "            \n",
                "    # Compute CM (Subset of top 20 most frequent tokens to make it readable)\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    \n",
                "    # Filter Top 20 tokens\n",
                "    unique, counts = np.unique(y_true, return_counts=True)\n",
                "    top_indices = np.argsort(counts)[::-1][:20]\n",
                "    # Keep indices valid\n",
                "    top_indices = [i for i in top_indices if i < len(unique)]\n",
                "    top_tokens = unique[top_indices]\n",
                "    \n",
                "    cm_subset = confusion_matrix(y_true, y_pred, labels=top_tokens)\n",
                "    \n",
                "    plt.figure(figsize=(12, 10))\n",
                "    sns.heatmap(cm_subset, annot=True, fmt='d', cmap='Blues', \n",
                "                xticklabels=top_tokens, yticklabels=top_tokens)\n",
                "    plt.title(f'Confusion Matrix (Top 20 Tokens) - {experiment}')\n",
                "    plt.ylabel('True Label')\n",
                "    plt.xlabel('Predicted Label')\n",
                "    plt.show()\n",
                "\n",
                "# Example: Plot CM for models\n",
                "try:\n",
                "    # Plot for first available non-baseline exp per dataset\n",
                "    for d in datasets:\n",
                "        plot_confusion_matrix(d, 'mor_exp1')\n",
                "except Exception as e:\n",
                "    print(f\"Could not plot confusion matrix: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}