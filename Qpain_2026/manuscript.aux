\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{b1,b2,b3}
\citation{b4,b5,b6}
\citation{b7}
\citation{b8,b9}
\citation{b10}
\citation{b11}
\citation{b12}
\citation{b10}
\citation{b11}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-A}}Background of The Research}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-B}}Problem Definition}{1}{subsection.1.2}\protected@file@percent }
\citation{b13}
\citation{b14}
\citation{b2}
\citation{b15}
\citation{b16}
\citation{b17}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-C}}Research Aim and Objectives}{2}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-D}}Contributions}{2}{subsection.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Evaluated Research Pipeline}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:proposed_method}{{1}{2}{Evaluated Research Pipeline}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Literature Review}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Introduction}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Large Language Model (LLM)}{2}{subsection.2.2}\protected@file@percent }
\citation{b18}
\citation{b19}
\citation{b20}
\citation{b21}
\citation{b22}
\citation{b23}
\citation{b24}
\citation{b12}
\citation{b12}
\citation{b12}
\citation{b12}
\citation{b25}
\citation{b11}
\citation{b26}
\citation{b11}
\citation{b27,b28}
\citation{b29}
\citation{b10}
\citation{b30}
\citation{b31}
\citation{b32}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Small Language Model (SLM)}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-D}}Transformer}{3}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Standard Transformer Architecture Diagram \cite  {b12}}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:transformer_arch}{{2}{3}{Standard Transformer Architecture Diagram \cite {b12}}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-E}}Recursive Transformer}{3}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-F}}Mixture of Experts}{3}{subsection.2.6}\protected@file@percent }
\citation{b11}
\citation{b12}
\citation{b11,b33}
\citation{b26}
\citation{b34}
\citation{b11}
\citation{b11}
\citation{b35}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-G}}Mixture of Recursion}{4}{subsection.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces MoR-Transformer Architecture Diagram \cite  {b11}}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:mor_transformer}{{3}{4}{MoR-Transformer Architecture Diagram \cite {b11}}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-H}}Research Gap}{4}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Dataset and Preprocessing}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}1}Dataset}{4}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}2}Tokenization}{4}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Tokenization and Granularities}}{4}{table.1}\protected@file@percent }
\newlabel{tab:tokenization}{{I}{4}{Tokenization and Granularities}{table.1}{}}
\citation{b12}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Research Pipeline}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Model Architecture}{5}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}1}Baseline Transformer}{5}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}2}MoR-Transformer}{5}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}3}Router Architecture}{5}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces MoR Routing Mechanism. The router predicts discrete actions (Skip, Forward, Recurse) for each token to optimize computational depth.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:routing_mechanism}{{4}{5}{MoR Routing Mechanism. The router predicts discrete actions (Skip, Forward, Recurse) for each token to optimize computational depth}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Experiments}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-E}}Training Configuration}{5}{subsection.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Training Configuration}}{5}{table.2}\protected@file@percent }
\newlabel{tab:training_config}{{II}{5}{Training Configuration}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-F}}Evaluation metrics}{5}{subsection.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-F}1}Effective Depth (E)}{5}{subsubsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-F}2}Accuracy}{5}{subsubsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-F}3}Held-out accuracy}{5}{subsubsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-F}4}Training Time}{5}{subsubsection.3.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Result and Analysis}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Results}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}1}Experiment 1: Efficiency Profiling (N=12 vs MoR N=12)}{6}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Experiment 1 result}}{6}{table.3}\protected@file@percent }
\newlabel{tab:exp1_result}{{III}{6}{Experiment 1 result}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Experiment 1: Efficiency Profiling Comparison between Standard Transformer (N=12) and MoR (N=12). MoR achieves identical accuracy with 33\% fewer layers and 23\% faster training.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:exp1_comparison}{{5}{6}{Experiment 1: Efficiency Profiling Comparison between Standard Transformer (N=12) and MoR (N=12). MoR achieves identical accuracy with 33\% fewer layers and 23\% faster training}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}2}Experiment 2: Performance Under Equivalent Cost (N=6 vs MoR N=12, E $\approx $ 6)}{6}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Experiment 2 result}}{6}{table.4}\protected@file@percent }
\newlabel{tab:exp2_result}{{IV}{6}{Experiment 2 result}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Experiment 2: Equal Cost Comparison. At equivalent computational cost (E $\approx $ 6), MoR achieves 34.77\% higher held-out accuracy than the shallow baseline.}}{6}{figure.6}\protected@file@percent }
\newlabel{fig:exp2_comparison}{{6}{6}{Experiment 2: Equal Cost Comparison. At equivalent computational cost (E $\approx $ 6), MoR achieves 34.77\% higher held-out accuracy than the shallow baseline}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}3}Experiment 3: Bangla Language Benchmark}{6}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Experiment 3 Result (Bangla)}}{6}{table.5}\protected@file@percent }
\newlabel{tab:exp3_result}{{V}{6}{Experiment 3 Result (Bangla)}{table.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}4}Experiment 4: High-Density \& Stability (WikiText-2)}{6}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Experiment 4 Result (WikiText-2)}}{7}{table.6}\protected@file@percent }
\newlabel{tab:exp4_result}{{VI}{7}{Experiment 4 Result (WikiText-2)}{table.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Stability Analysis across WikiText-2 and Bangla benchmarks. While standard 12-layer architectures diverge (WikiText) or fail to converge (Bangla) under standard regimes, MoR restores architectural stability through dynamic execution paths.}}{7}{figure.7}\protected@file@percent }
\newlabel{fig:stability_analysis}{{7}{7}{Stability Analysis across WikiText-2 and Bangla benchmarks. While standard 12-layer architectures diverge (WikiText) or fail to converge (Bangla) under standard regimes, MoR restores architectural stability through dynamic execution paths}{figure.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Overall Outcome}}{7}{table.7}\protected@file@percent }
\newlabel{tab:overall_outcome}{{VII}{7}{Overall Outcome}{table.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}5}Overall Experimental Outcome}{7}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Result Analysis}{7}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Performance Summary Across Token Granularities. MoR demonstrates consistent efficiency gains across character-level (Shakespeare), morphological subword (Bangla), and high-density subword (WikiText-2) tokenization.}}{7}{figure.8}\protected@file@percent }
\newlabel{fig:multi_granularity}{{8}{7}{Performance Summary Across Token Granularities. MoR demonstrates consistent efficiency gains across character-level (Shakespeare), morphological subword (Bangla), and high-density subword (WikiText-2) tokenization}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Computational Savings Across Datasets. MoR consistently achieves approximately 33\% reduction in effective depth across all three datasets.}}{7}{figure.9}\protected@file@percent }
\newlabel{fig:computational_savings}{{9}{7}{Computational Savings Across Datasets. MoR consistently achieves approximately 33\% reduction in effective depth across all three datasets}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Learning Trajectories on Bangla Benchmark. The dual-subplot visualization shows (a) Training Loss and (b) Training Accuracy across 10 epochs. The comparison includes both deep (N=12) and shallow (N=6) standard baselines alongside MoR experiments. While the larger 12-layer baseline leverages its capacity for higher performance headroom, the 6-layer baseline and MoR variants demonstrate the efficiency-stability trade-off under the optimized regime.}}{7}{figure.10}\protected@file@percent }
\newlabel{fig:learning_curves}{{10}{7}{Learning Trajectories on Bangla Benchmark. The dual-subplot visualization shows (a) Training Loss and (b) Training Accuracy across 10 epochs. The comparison includes both deep (N=12) and shallow (N=6) standard baselines alongside MoR experiments. While the larger 12-layer baseline leverages its capacity for higher performance headroom, the 6-layer baseline and MoR variants demonstrate the efficiency-stability trade-off under the optimized regime}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Efficiency-Performance Trade-off. MoR models (blue markers) consistently occupy the upper-left quadrant, achieving higher accuracy at lower effective depths compared to deep fixed baselines.}}{8}{figure.11}\protected@file@percent }
\newlabel{fig:tradeoff}{{11}{8}{Efficiency-Performance Trade-off. MoR models (blue markers) consistently occupy the upper-left quadrant, achieving higher accuracy at lower effective depths compared to deep fixed baselines}{figure.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion \& Future Work}{8}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Conclusion}{8}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Limitations}{8}{subsection.5.2}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{manuscript_ref}
\bibcite{b1}{1}
\bibcite{b2}{2}
\bibcite{b3}{3}
\bibcite{b4}{4}
\bibcite{b5}{5}
\bibcite{b6}{6}
\bibcite{b7}{7}
\bibcite{b8}{8}
\bibcite{b9}{9}
\bibcite{b10}{10}
\bibcite{b11}{11}
\bibcite{b12}{12}
\bibcite{b13}{13}
\bibcite{b14}{14}
\bibcite{b15}{15}
\bibcite{b16}{16}
\bibcite{b17}{17}
\bibcite{b18}{18}
\bibcite{b19}{19}
\bibcite{b20}{20}
\bibcite{b21}{21}
\bibcite{b22}{22}
\bibcite{b23}{23}
\bibcite{b24}{24}
\bibcite{b25}{25}
\bibcite{b26}{26}
\bibcite{b27}{27}
\bibcite{b28}{28}
\bibcite{b29}{29}
\bibcite{b30}{30}
\bibcite{b31}{31}
\bibcite{b32}{32}
\bibcite{b33}{33}
\bibcite{b34}{34}
\bibcite{b35}{35}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}Future Works}{9}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{9}{section*.1}\protected@file@percent }
\gdef \@abspage@last{9}
