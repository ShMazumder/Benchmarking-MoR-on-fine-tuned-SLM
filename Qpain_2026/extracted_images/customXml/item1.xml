<?xml version="1.0" encoding="UTF-8" standalone="no"?><b:Sources xmlns:b="http://schemas.openxmlformats.org/officeDocument/2006/bibliography" xmlns="http://schemas.openxmlformats.org/officeDocument/2006/bibliography" SelectedStyle="\IEEE2006OfficeOnline.xsl" StyleName="IEEE" Version="2006"><b:Source><b:Tag>Ash17</b:Tag><b:SourceType>Report</b:SourceType><b:Guid>{A830ADCB-F8D1-4AD5-BA17-E62B565FCDC0}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Ashish Vaswani</b:Last><b:First>Noam</b:First><b:Middle>Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Attention Is All You Need</b:Title><b:Year>2017</b:Year><b:DOI>https://doi.org/10.48550/arXiv.1706.03762</b:DOI><b:RefOrder>12</b:RefOrder></b:Source><b:Source><b:Tag>San25</b:Tag><b:SourceType>Report</b:SourceType><b:Guid>{2EEBC6AD-9CE8-47A9-985E-3953E50D6D1F}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Sangmin Bae</b:Last><b:First>Yujin</b:First><b:Middle>Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</b:Title><b:Year>2025</b:Year><b:DOI>https://doi.org/10.48550/arXiv.2507.10524</b:DOI><b:RefOrder>11</b:RefOrder></b:Source><b:Source><b:Tag>Jac191</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{1A5C75FA-87DB-4BF3-8978-A695F099C33F}</b:Guid><b:Title>BERT: Pre-training of deep bidirectional transformers for language understanding</b:Title><b:Year>2019</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Jacob Devlin</b:Last><b:First>Ming-Wei</b:First><b:Middle>Chang, Kenton Lee, Kristina Toutanova</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Pages>4171–4186</b:Pages><b:Volume>2</b:Volume><b:RefOrder>1</b:RefOrder></b:Source><b:Source><b:Tag>Col20</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{C92B27C5-A969-4497-8562-0C785EF698B6}</b:Guid><b:Title>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</b:Title><b:Year>2020</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Colin Raffel</b:Last><b:First>Noam</b:First><b:Middle>Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:JournalName>Journal of Machine Learning Research</b:JournalName><b:Pages>1-67</b:Pages><b:Volume>21</b:Volume><b:Issue>140</b:Issue><b:RefOrder>3</b:RefOrder></b:Source><b:Source><b:Tag>Zih19</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{5B00FB64-5D39-4E7E-AC89-056914750747}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Zihang Dai</b:Last><b:First>Zhilin</b:First><b:Middle>Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</b:Title><b:Year>2019</b:Year><b:Pages>2978-2988</b:Pages><b:RefOrder>4</b:RefOrder></b:Source><b:Source><b:Tag>Nik20</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{63CBC46D-3DB9-46B0-87CD-1B39810705E5}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Nikita Kitaev</b:Last><b:First>Łukasz</b:First><b:Middle>Kaiser, Anselm Levskaya</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Reformer: The Efficient Transformer</b:Title><b:Year>2020</b:Year><b:RefOrder>5</b:RefOrder></b:Source><b:Source><b:Tag>Jac20</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{90F476FC-7232-4460-88FF-37272E061E97}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Jack W. Rae</b:Last><b:First>Anna</b:First><b:Middle>Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Compressive Transformers for Long-Range Sequence Modelling</b:Title><b:Year>2020</b:Year><b:RefOrder>6</b:RefOrder></b:Source><b:Source><b:Tag>Mic24</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{BD939170-F0B1-4207-84F7-3DA0AD320C40}</b:Guid><b:Title>Towards Understanding the Universality of Transformers for Next-Token Prediction</b:Title><b:Year>2024</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Michael E. Sander</b:Last><b:First>Gabriel</b:First><b:Middle>Peyré</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:RefOrder>9</b:RefOrder></b:Source><b:Source xmlns:b="http://schemas.openxmlformats.org/officeDocument/2006/bibliography"><b:Tag>Bro20</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{38FAB505-2461-4C3E-A0BF-FF832802EF61}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Brown</b:Last><b:First>Tom</b:First><b:Middle>B. et al.</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Language Models are Few-Shot Learners</b:Title><b:Year>2020</b:Year><b:Pages>1877-1901</b:Pages><b:Volume>33</b:Volume><b:RefOrder>2</b:RefOrder></b:Source><b:Source><b:Tag>Yin24</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{7DB9AF1B-ED9B-4413-8418-AFB1071DC411}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Ildiz</b:Last><b:First>Yingcong</b:First><b:Middle>Li and Yixiao Huang and M. Emrullah</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Mechanics of Next Token Prediction with Self-Attention</b:Title><b:Year>2024</b:Year><b:RefOrder>8</b:RefOrder></b:Source><b:Source><b:Tag>Noa17</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{49B44F4A-96F0-4460-AF69-DE2DC987FB87}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Noam Shazeer</b:Last><b:First>Azalia</b:First><b:Middle>Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</b:Title><b:Year>2017</b:Year><b:RefOrder>10</b:RefOrder></b:Source><b:Source><b:Tag>Lou25</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{9F770A66-BEC9-45DF-B2BE-BA9C9F383411}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>others</b:Last><b:First>Loubna</b:First><b:Middle>Ben Allal and</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model</b:Title><b:Year>2025</b:Year><b:RefOrder>7</b:RefOrder></b:Source><b:Source><b:Tag>Hum23</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{A8B0B2BC-26FB-4524-9FA5-283FAE4BF707}</b:Guid><b:Title>A Comprehensive Overview of Large Language Models</b:Title><b:Year>2023</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Humza Naveed</b:Last><b:First>Asad</b:First><b:Middle>Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:JournalName>arXiv</b:JournalName><b:Pages>194</b:Pages><b:RefOrder>13</b:RefOrder></b:Source><b:Source><b:Tag>Ale18</b:Tag><b:SourceType>Report</b:SourceType><b:Guid>{48D9C717-D8CA-4B54-9908-0F83C27D52C0}</b:Guid><b:Title>Improving Language Understanding by Generative Pre-Training</b:Title><b:Year>2018</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Sutskever</b:Last><b:First>Alec</b:First><b:Middle>Radford and Karthik Narasimhan and Tim Salimans and Ilya</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:RefOrder>14</b:RefOrder></b:Source><b:Source><b:Tag>Jar20</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{F61F9410-E145-4BF1-95F2-9A51A3A464AD}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Jared Kaplan</b:Last><b:First>Sam</b:First><b:Middle>McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Scaling Laws for Neural Language Models</b:Title><b:Year>2020</b:Year><b:JournalName>arXiv preprint arXiv:2001.08361</b:JournalName><b:RefOrder>15</b:RefOrder></b:Source><b:Source><b:Tag>oth23</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{44498B6E-6C56-4BE3-AF98-5718A4D99D2E}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>others</b:Last><b:First>Aakanksha</b:First><b:Middle>Chowdhery and Sharan Narang and Jacob Devlin and</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>PaLM: Scaling Language Modeling with Pathways</b:Title><b:JournalName>Journal of Machine Learning Research</b:JournalName><b:Year>2023</b:Year><b:Pages>1-113</b:Pages><b:Volume>24</b:Volume><b:Issue>240</b:Issue><b:RefOrder>16</b:RefOrder></b:Source><b:Source><b:Tag>Ope23</b:Tag><b:SourceType>Report</b:SourceType><b:Guid>{45D8FBF1-85E6-4A37-BC65-1E60E415DC2C}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>OpenAI</b:Last></b:Person></b:NameList></b:Author></b:Author><b:Title>GPT-4 Technical Report</b:Title><b:Year>2023</b:Year><b:Institution>OpenAI</b:Institution><b:RefOrder>17</b:RefOrder></b:Source><b:Source><b:Tag>Jok25</b:Tag><b:SourceType>InternetSite</b:SourceType><b:Guid>{803B1E94-9D0A-4200-A0FA-5C55A1157734}</b:Guid><b:Title>Small Language Models (SLM): A Comprehensive Overview</b:Title><b:Year>2025</b:Year><b:ProductionCompany>Hugging Face</b:ProductionCompany><b:Month>feb</b:Month><b:URL>https://huggingface.co/blog/jjokah/small-language-model</b:URL><b:Author><b:Author><b:NameList><b:Person><b:Last>Jokah</b:Last><b:First>James</b:First></b:Person></b:NameList></b:Author></b:Author><b:RefOrder>18</b:RefOrder></b:Source><b:Source><b:Tag>Qin25</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{7BDD535D-6B68-48FF-8FC6-BD7C7C157D89}</b:Guid><b:Title>The Rise of Small Language Models</b:Title><b:Year>2025</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>others</b:Last><b:First>Qin</b:First><b:Middle>Zhang and</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:JournalName>IEEE Intelligent Systems</b:JournalName><b:Pages>30-37</b:Pages><b:Volume>40</b:Volume><b:Issue>1</b:Issue><b:DOI>10.1109/MIS.2024.3517792</b:DOI><b:RefOrder>19</b:RefOrder></b:Source><b:Source><b:Tag>Zhe25</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{F1A52D09-59D6-4214-A4F3-78F0D113E3D2}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Zhenyan Lu</b:Last><b:First>Xiang</b:First><b:Middle>Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Small Language Models: Survey, Measurements, and Insights</b:Title><b:JournalName>arXiv</b:JournalName><b:Year>2025</b:Year><b:DOI>10.48550/arXiv.2409.15790</b:DOI><b:RefOrder>20</b:RefOrder></b:Source><b:Source><b:Tag>Vic19</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{F7F4CE72-C8CF-458C-B6F8-12BAF5326048}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Victor Sanh</b:Last><b:First>Lysandre</b:First><b:Middle>Debut, Julien Chaumond, Thomas Wolf</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</b:Title><b:JournalName>arXiv</b:JournalName><b:Year>2019</b:Year><b:DOI>10.48550/arXiv.1910.01108</b:DOI><b:RefOrder>21</b:RefOrder></b:Source><b:Source><b:Tag>oth231</b:Tag><b:SourceType>Report</b:SourceType><b:Guid>{8F40745C-49F0-4498-9AB4-C8EEE4D9617D}</b:Guid><b:Title>Textbooks Are All You Need</b:Title><b:Year>2023</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>others</b:Last><b:First>Suriya</b:First><b:Middle>Gunasekar and</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:DOI>10.48550/arXiv.2306.11644</b:DOI><b:RefOrder>22</b:RefOrder></b:Source><b:Source><b:Tag>Moj23</b:Tag><b:SourceType>InternetSite</b:SourceType><b:Guid>{A54CD121-2412-4889-8177-9BED8D844E17}</b:Guid><b:Title>Phi-2: The Surprising Power of Small Language Models</b:Title><b:Year>2023</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Mojan Javaheripi</b:Last><b:First>Principal</b:First><b:Middle>Researcher Sébastien Bubeck , Vice President, Microsoft GenAI</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:ProductionCompany>Microsoft</b:ProductionCompany><b:URL>https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</b:URL><b:RefOrder>23</b:RefOrder></b:Source><b:Source><b:Tag>oth24</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{970BB283-90C9-4C34-B4AE-F6EA6D73EC5C}</b:Guid><b:Title>Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</b:Title><b:Year>2024</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>others</b:Last><b:First>Marah</b:First><b:Middle>Abdin and</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:JournalName>arXiv</b:JournalName><b:DOI>10.48550/arXiv.2404.14219</b:DOI><b:RefOrder>24</b:RefOrder></b:Source><b:Source><b:Tag>San251</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{91067F5A-81D9-46FD-AF17-CC723E8EE787}</b:Guid><b:Title>Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA</b:Title><b:Year>2025</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Sangmin Bae</b:Last><b:First>Adam</b:First><b:Middle>Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:ConferenceName>International Conference on Learning Representations (ICLR)</b:ConferenceName><b:DOI>10.48550/arXiv.2410.20672</b:DOI><b:RefOrder>25</b:RefOrder></b:Source><b:Source><b:Tag>Mos19</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{17551B60-88C1-4123-8748-02975B30E3AA}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Mostafa Dehghani</b:Last><b:First>Stephan</b:First><b:Middle>Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Universal Transformers</b:Title><b:Year>2019</b:Year><b:ConferenceName>International Conference on Learning Representations (ICLR)</b:ConferenceName><b:DOI>10.48550/arXiv.1807.03819</b:DOI><b:RefOrder>26</b:RefOrder></b:Source><b:Source><b:Tag>Wan24</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{98C06808-76ED-488D-9033-69BB2FC75DA8}</b:Guid><b:Title>A Survey on Mixture of Experts in Large Language Models</b:Title><b:Year>2024</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Wang</b:Last><b:First>Weilin</b:First><b:Middle>Cai and Juyong Jiang and Fan</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:JournalName>IEEE Transactions on Knowledge and Data Engineering</b:JournalName><b:Pages>7304-7317</b:Pages><b:Volume>36</b:Volume><b:Issue>12</b:Issue><b:Month>December</b:Month><b:DOI>10.1109/TKDE.2024.3449667</b:DOI><b:RefOrder>27</b:RefOrder></b:Source><b:Source><b:Tag>Yan22</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{8AFBB45A-0F0B-4B4E-8918-BCA32550F1FF}</b:Guid><b:Title>Mixture-of-Experts with Expert Choice Routing</b:Title><b:Year>2022</b:Year><b:Pages>7103-7114</b:Pages><b:Volume>35</b:Volume><b:Author><b:Author><b:NameList><b:Person><b:Last>Yanqi Zhou</b:Last><b:First>Tao</b:First><b:Middle>Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, James Laudon</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:DOI>10.48550/arXiv.2202.09368</b:DOI><b:RefOrder>28</b:RefOrder></b:Source><b:Source><b:Tag>Jac91</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{13AC3BD2-FA92-44B4-801B-4D8CD5EBF0E3}</b:Guid><b:Title>Adaptive Mixtures of Local Experts</b:Title><b:Year>1991</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Jacobs</b:Last><b:First>Robert</b:First><b:Middle>A.</b:Middle></b:Person><b:Person><b:Last>Jordan</b:Last><b:First>Michael</b:First><b:Middle>I.</b:Middle></b:Person><b:Person><b:Last>Nowlan</b:Last><b:First>Steven</b:First><b:Middle>J.</b:Middle></b:Person><b:Person><b:Last>Hinton</b:Last><b:First>Geoffrey</b:First><b:Middle>E.</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:JournalName>Neural Computation</b:JournalName><b:Pages>79-87</b:Pages><b:Volume>3</b:Volume><b:Issue>1</b:Issue><b:DOI>10.1162/neco.1991.3.1.79</b:DOI><b:RefOrder>29</b:RefOrder></b:Source><b:Source><b:Tag>Wil22</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{20AA996C-85B0-4D80-8E1F-315C23356D2E}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Shazeer</b:Last><b:First>William</b:First><b:Middle>Fedus and Barret Zoph and Noam</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Switch Transformers: Scaling to Trillion Parameter Models with Simple and E cient Sparsity</b:Title><b:JournalName>Journal of Machine Learning Research</b:JournalName><b:Year>2022</b:Year><b:Pages>1-39</b:Pages><b:Volume>23</b:Volume><b:RefOrder>30</b:RefOrder></b:Source><b:Source><b:Tag>alA24</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{50A4317B-09D1-4F22-A63D-E0909BE03684}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>al.</b:Last><b:First>Albert</b:First><b:Middle>Q. Jiang et</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Mixtral of Experts</b:Title><b:JournalName>arXiv</b:JournalName><b:Year>2024</b:Year><b:DOI>10.48550/arXiv.2401.04088</b:DOI><b:RefOrder>31</b:RefOrder></b:Source><b:Source><b:Tag>alD24</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{219915BE-CFC4-429C-BD96-10E67B767E42}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>al.</b:Last><b:First>DeepSeek-AI</b:First><b:Middle>et</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</b:Title><b:JournalName>arXiv</b:JournalName><b:Year>2024</b:Year><b:DOI>10.48550/arXiv.2405.04434</b:DOI><b:RefOrder>32</b:RefOrder></b:Source><b:Source><b:Tag>oth22</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{9FC4527E-94C0-4911-8EA2-2AB5FD93041C}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>others</b:Last><b:First>Zhanghui</b:First><b:Middle>Shen and</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Sliced Recursive Transformer</b:Title><b:Year>2022</b:Year><b:DOI>10.48550/arXiv.2111.05297</b:DOI><b:RefOrder>33</b:RefOrder></b:Source><b:Source><b:Tag>Mah20</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{E99ECFF7-66A8-4060-BB35-0C47A521393E}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Maha Elbayad</b:Last><b:First>Jiatao</b:First><b:Middle>Gu, Edouard Grave, Michael Auli</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Depth-Adaptive Transformer</b:Title><b:Year>2020</b:Year><b:ConferenceName>International Conference on Learning Representations (ICLR)</b:ConferenceName><b:URL>https://openreview.net/forum?id=SJg7KhVKPH</b:URL><b:DOI>10.48550/arXiv.1910.10073</b:DOI><b:RefOrder>34</b:RefOrder></b:Source></b:Sources>