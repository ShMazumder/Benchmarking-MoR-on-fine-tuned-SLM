@article{b1,
  title   = {{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author  = {J. Devlin and M.-W. Chang and K. Lee and K. Toutanova},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018}
}

@inproceedings{b2,
  title     = {Language models are few-shot learners},
  author    = {T. Brown and B. Mann and N. Ryder and others},
  booktitle = {Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)},
  volume    = {33},
  pages     = {1877--1901},
  year      = {2020}
}

@article{b3,
  title   = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author  = {C. Raffel and N. Shazeer and A. Roberts and others},
  journal = {J. Mach. Learn. Res.},
  volume  = {21},
  number  = {1},
  pages   = {5485--5551},
  year    = {2020}
}

@inproceedings{b4,
  title     = {Transformer-{XL}: Attentive language models beyond a fixed-length context},
  author    = {Z. Dai and Z. Yang and Y. Yang and J. Carbonell and Q. V. Le and R. Salakhutdinov},
  booktitle = {Proc. 57th Annu. Meet. Assoc. Comput. Linguist. (ACL)},
  pages     = {2978--2988},
  year      = {2019}
}

@inproceedings{b5,
  title     = {Reformer: The Efficient Transformer},
  author    = {N. Kitaev and L. Kaiser and A. Levskaya},
  booktitle = {Proc. Int. Conf. Learn. Represent. (ICLR)},
  year      = {2020}
}

@article{b6,
  title   = {Compressive Transformers for Long-Range Sequence Modelling},
  author  = {J. W. Rae and A. Potapenko and S. M. Jayakumar and C. Hillier and T. P. Lillicrap},
  journal = {arXiv preprint arXiv:1911.05507},
  year    = {2019}
}

@article{b7,
  title   = {{SmolLM2}: When Smol Goes Big--Data-Centric Training of a Small Language Model},
  author  = {L. Ben Allal and others},
  journal = {arXiv preprint},
  year    = {2025}
}

@article{b8,
  title   = {Mechanics of Next Token Prediction with Self-Attention},
  author  = {Y. Li and Y. Huang and M. E. Ildiz and A. S. Rawat and S. Oymak},
  journal = {arXiv preprint},
  year    = {2024}
}

@article{b9,
  title   = {Towards Understanding the Universality of Transformers for Next-Token Prediction},
  author  = {M. E. Sander and G. Peyr{\'e}},
  journal = {arXiv preprint arXiv:2410.03011},
  year    = {2024}
}

@inproceedings{b10,
  title     = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author    = {N. Shazeer and A. Mirhoseini and K. Maziarz and A. Davis and Q. Le and G. Hinton and J. Dean},
  booktitle = {Proc. Int. Conf. Learn. Represent. (ICLR)},
  year      = {2017}
}

@article{b11,
  author  = {Sangmin Bae and Yujin Kim and Reza Bayat and Sungnyun Kim and Jiyoun Ha and Tal Schuster and Adam Fisch and Hrayr Harutyunyan and Ziwei Ji and Aaron Courville and Se-Young Yun},
  title   = {Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation},
  journal = {arXiv preprint arXiv:2507.10524},
  year    = {2025},
  note    = {Accepted to NeurIPS 2025},
  url     = {https://arxiv.org/abs/2507.10524}
}

@inproceedings{b12,
  title     = {Attention is all you need},
  author    = {A. Vaswani and N. Shazeer and N. Parmar and J. Uszkoreit and L. Jones and A. N. Gomez and L. Kaiser and I. Polosukhin},
  booktitle = {Proc. Adv. Neural Inf. Process. Syst. (NIPS)},
  pages     = {5998--6008},
  year      = {2017}
}

@article{b13,
  title   = {A comprehensive overview of large language models},
  author  = {H. Naveed and others},
  journal = {arXiv preprint},
  year    = {2023}
}

@techreport{b14,
  title       = {Improving language understanding by generative pre-training},
  author      = {A. Radford and K. Narasimhan and T. Salimans and I. Sutskever},
  year        = {2018},
  institution = {OpenAI}
}

@article{b15,
  title   = {Scaling laws for neural language models},
  author  = {J. Kaplan and S. McCandlish and T. Henighan and T. B. Brown and B. Chess and R. Child and S. Gray and A. Radford and J. Wu and D. Amodei},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = {2020}
}

@article{b16,
  title   = {{PaLM}: Scaling language modeling with pathways},
  author  = {A. Chowdhery and others},
  journal = {J. Mach. Learn. Res.},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
  year    = {2023}
}

@article{b17,
  title   = {{GPT}-4 technical report},
  author  = {OpenAI},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}

@misc{b18,
  title        = {Small language models ({SLM}): A comprehensive overview},
  author       = {J. Okah},
  year         = {2025},
  howpublished = {Hugging Face}
}

@article{b19,
  title   = {The rise of small language models},
  author  = {Q. Zhang and others},
  journal = {IEEE Intell. Syst.},
  volume  = {40},
  number  = {1},
  pages   = {30--37},
  year    = {2025}
}

@article{b20,
  title   = {Small language models: Survey, measurements, and insights},
  author  = {Z. Lu and others},
  journal = {arXiv preprint},
  year    = {2025}
}

@article{b21,
  title   = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author  = {V. Sanh and L. Debut and J. Chaumond and T. Wolf},
  journal = {arXiv preprint arXiv:1910.01108},
  year    = {2019}
}

@article{b22,
  title   = {Textbooks are all you need},
  author  = {S. Gunasekar and Y. Zhang and J. Aneja and C. C. Teodoro Mendes and others},
  journal = {arXiv preprint arXiv:2306.11644},
  year    = {2023}
}

@techreport{b23,
  title       = {Phi-2: The surprising power of small language models},
  author      = {M. Javaheripi and others},
  institution = {Microsoft},
  year        = {2023}
}

@article{b24,
  title   = {Phi-3 technical report: A highly capable language model locally on your phone},
  author  = {M. Abdin and others},
  journal = {arXiv preprint arXiv:2404.14219},
  year    = {2024}
}

@inproceedings{b25,
  title     = {Relaxed recursive transformers: Effective parameter sharing with layer-wise {LoRA}},
  author    = {S. Bae and A. Fisch and H. Harutyunyan and Z. Ji and S. Kim and T. Schuster},
  booktitle = {Proc. 13th Int. Conf. Learn. Represent. (ICLR)},
  year      = {2025}
}

@inproceedings{b26,
  title     = {Universal transformers},
  author    = {M. Dehghani and S. Gouws and O. Vinyals and J. Uszkoreit and L. Kaiser},
  booktitle = {Proc. Int. Conf. Learn. Represent. (ICLR)},
  year      = {2019}
}

@article{b27,
  title   = {A survey on mixture of experts in large language models},
  author  = {W. Cai and others},
  journal = {IEEE Trans. Knowl. Data Eng.},
  volume  = {36},
  number  = {12},
  pages   = {7304--7317},
  year    = {2024}
}

@inproceedings{b28,
  title     = {Mixture-of-experts with expert choice routing},
  author    = {Y. Zhou and others},
  booktitle = {Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)},
  volume    = {35},
  pages     = {7103--7114},
  year      = {2022}
}

@article{b29,
  title   = {Adaptive mixtures of local experts},
  author  = {R. A. Jacobs and M. I. Jordan and S. J. Nowlan and G. E. Hinton},
  journal = {Neural Comput.},
  volume  = {3},
  number  = {1},
  pages   = {79--87},
  year    = {1991}
}

@article{b30,
  title   = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author  = {W. Fedus and B. Zoph and N. Shazeer},
  journal = {J. Mach. Learn. Res.},
  volume  = {23},
  number  = {1},
  pages   = {5232--5270},
  year    = {2022}
}

@article{b31,
  title   = {Mixtral of experts},
  author  = {A. Q. Jiang and others},
  journal = {arXiv preprint arXiv:2401.04088},
  year    = {2024}
}

@article{b32,
  title   = {{DeepSeek}-V2: A strong, economical, and efficient mixture-of-experts language model},
  author  = {DeepSeek-AI},
  journal = {arXiv preprint arXiv:2405.04434},
  year    = {2024}
}

@article{b33,
  title   = {Sliced recursive transformer},
  author  = {S. Shen and others},
  journal = {arXiv preprint},
  year    = {2022}
}

@inproceedings{b34,
  title     = {Depth-adaptive transformer},
  author    = {M. Elbayad and J. Gu and E. Grave and M. Auli},
  booktitle = {Proc. Int. Conf. Learn. Represent. (ICLR)},
  year      = {2020}
}

@misc{b35,
  title        = {char-rnn},
  author       = {A. Karpathy},
  year         = {2015},
  howpublished = {GitHub repository},
  note         = {\url{https://github.com/karpathy/char-rnn}}
}
