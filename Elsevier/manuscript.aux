\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{b11}
\Newlabel{1}{a}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Research Background}{1}{subsection.1.1}\protected@file@percent }
\citation{b1,b2,b3}
\citation{b4,b5,b6}
\citation{b7}
\citation{b8,b9}
\citation{b10}
\citation{b11}
\citation{b12}
\citation{b10}
\citation{b11}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Problem Definition}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Research Aim and Objectives}{4}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Contributions}{4}{subsection.1.4}\protected@file@percent }
\citation{b13}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Research Pipeline}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:proposed_method}{{1}{5}{Research Pipeline}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Introduction}{5}{subsection.2.1}\protected@file@percent }
\citation{b14}
\citation{b2}
\citation{b15}
\citation{b16}
\citation{b17}
\citation{b18}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Large Language Model (LLM)}{6}{subsection.2.2}\protected@file@percent }
\citation{b19}
\citation{b20}
\citation{b21}
\citation{b22}
\citation{b23}
\citation{b24}
\citation{b12}
\citation{b12}
\citation{b12}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Small Language Model (SLM)}{7}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Transformer}{7}{subsection.2.4}\protected@file@percent }
\citation{b12}
\citation{b25}
\citation{b11}
\citation{b26}
\citation{b11}
\citation{b27,b28}
\citation{b29}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Standard Transformer Architecture Diagram \cite  {b12}}}{8}{figure.2}\protected@file@percent }
\newlabel{fig:transformer_arch}{{2}{8}{Standard Transformer Architecture Diagram \cite {b12}}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Recursive Transformer}{8}{subsection.2.5}\protected@file@percent }
\citation{b10}
\citation{b30}
\citation{b31}
\citation{b32}
\citation{b11}
\citation{b12}
\citation{b11,b33}
\citation{b26}
\citation{b34}
\citation{b11}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Mixture of Experts}{9}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Mixture of Recursion}{9}{subsection.2.7}\protected@file@percent }
\citation{b11}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces MoR-Transformer Architecture Diagram \cite  {b11}}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:mor_transformer}{{3}{10}{MoR-Transformer Architecture Diagram \cite {b11}}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Research Gap}{10}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{10}{section.3}\protected@file@percent }
\citation{b35}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset and Preprocessing}{11}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Dataset}{11}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Tokenization}{11}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Tokenization and Granularities}}{11}{table.1}\protected@file@percent }
\newlabel{tab:tokenization}{{1}{11}{Tokenization and Granularities}{table.1}{}}
\citation{b12}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Research Pipeline}{12}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Model Architecture}{12}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Baseline Transformer}{12}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}MoR-Transformer}{12}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Router Architecture}{12}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces MoR Routing Mechanism. The router predicts discrete actions (Skip, Forward, Recurse) for each token to optimize computational depth.}}{13}{figure.4}\protected@file@percent }
\newlabel{fig:routing_mechanism}{{4}{13}{MoR Routing Mechanism. The router predicts discrete actions (Skip, Forward, Recurse) for each token to optimize computational depth}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experiments}{13}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Training Configuration}{13}{subsection.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training Configuration}}{14}{table.2}\protected@file@percent }
\newlabel{tab:training_config}{{2}{14}{Training Configuration}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Evaluation metrics}{15}{subsection.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1}Effective Depth (E)}{15}{subsubsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2}Accuracy}{15}{subsubsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.3}Held-out accuracy}{15}{subsubsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.4}Training Time}{15}{subsubsection.3.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Result and Analysis}{15}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Results}{15}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Experiment 1 result}}{16}{table.3}\protected@file@percent }
\newlabel{tab:exp1_result}{{3}{16}{Experiment 1 result}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Experiment 1: Efficiency Profiling (N=12 vs MoR N=12)}{16}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Experiment 1: Efficiency Profiling Comparison between Standard Transformer (N=12) and MoR (N=12). MoR achieves identical accuracy with 33\% fewer layers and 23\% faster training.}}{16}{figure.5}\protected@file@percent }
\newlabel{fig:exp1_comparison}{{5}{16}{Experiment 1: Efficiency Profiling Comparison between Standard Transformer (N=12) and MoR (N=12). MoR achieves identical accuracy with 33\% fewer layers and 23\% faster training}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Experiment 2: Performance Under Equivalent Cost (N=6 vs MoR N=12, E $\approx $ 6)}{16}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Experiment 2 result}}{17}{table.4}\protected@file@percent }
\newlabel{tab:exp2_result}{{4}{17}{Experiment 2 result}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Experiment 2: Equal Cost Comparison. At equivalent computational cost (E $\approx $ 6), MoR achieves 34.77\% higher held-out accuracy than the shallow baseline.}}{17}{figure.6}\protected@file@percent }
\newlabel{fig:exp2_comparison}{{6}{17}{Experiment 2: Equal Cost Comparison. At equivalent computational cost (\texorpdfstring {E $\approx $ 6}{E approx 6}), MoR achieves 34.77\% higher held-out accuracy than the shallow baseline}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Experiment 3: Bangla Language Benchmark}{17}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Experiment 3 Result (Bangla)}}{18}{table.5}\protected@file@percent }
\newlabel{tab:exp3_result}{{5}{18}{Experiment 3 Result (Bangla)}{table.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Experiment 4: High-Density \& Stability (WikiText-2)}{18}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Experiment 4 Result (WikiText-2)}}{18}{table.6}\protected@file@percent }
\newlabel{tab:exp4_result}{{6}{18}{Experiment 4 Result (WikiText-2)}{table.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Stability Analysis across WikiText-2 and Bangla benchmarks. While standard 12-layer architectures diverge (WikiText) or fail to converge (Bangla) under standard regimes, MoR restores architectural stability through dynamic execution paths.}}{19}{figure.7}\protected@file@percent }
\newlabel{fig:stability_analysis}{{7}{19}{Stability Analysis across WikiText-2 and Bangla benchmarks. While standard 12-layer architectures diverge (WikiText) or fail to converge (Bangla) under standard regimes, MoR restores architectural stability through dynamic execution paths}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5}Overall Experimental Outcome}{19}{subsubsection.4.1.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Overall Outcome}}{19}{table.7}\protected@file@percent }
\newlabel{tab:overall_outcome}{{7}{19}{Overall Outcome}{table.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Performance Summary Across Token Granularities. MoR demonstrates consistent efficiency gains across character-level (Shakespeare), morphological subword (Bangla), and high-density subword (WikiText-2) tokenization.}}{20}{figure.8}\protected@file@percent }
\newlabel{fig:multi_granularity}{{8}{20}{Performance Summary Across Token Granularities. MoR demonstrates consistent efficiency gains across character-level (Shakespeare), morphological subword (Bangla), and high-density subword (WikiText-2) tokenization}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Computational Savings Across Datasets. MoR consistently achieves approximately 33\% reduction in effective depth across all three datasets.}}{20}{figure.9}\protected@file@percent }
\newlabel{fig:computational_savings}{{9}{20}{Computational Savings Across Datasets. MoR consistently achieves approximately 33\% reduction in effective depth across all three datasets}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Learning Trajectories on Bangla Benchmark. The dual-subplot visualization shows (a) Training Loss and (b) Training Accuracy across 10 epochs. The comparison includes both deep (N=12) and shallow (N=6) standard baselines alongside MoR experiments. While the larger 12-layer baseline leverages its capacity for higher performance headroom, the 6-layer baseline and MoR variants demonstrate the efficiency-stability trade-off under the optimized regime.}}{21}{figure.10}\protected@file@percent }
\newlabel{fig:learning_curves}{{10}{21}{Learning Trajectories on Bangla Benchmark. The dual-subplot visualization shows (a) Training Loss and (b) Training Accuracy across 10 epochs. The comparison includes both deep (N=12) and shallow (N=6) standard baselines alongside MoR experiments. While the larger 12-layer baseline leverages its capacity for higher performance headroom, the 6-layer baseline and MoR variants demonstrate the efficiency-stability trade-off under the optimized regime}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Efficiency-Performance Trade-off. MoR models (blue markers) consistently occupy the upper-left quadrant, achieving higher accuracy at lower effective depths compared to deep fixed baselines.}}{21}{figure.11}\protected@file@percent }
\newlabel{fig:tradeoff}{{11}{21}{Efficiency-Performance Trade-off. MoR models (blue markers) consistently occupy the upper-left quadrant, achieving higher accuracy at lower effective depths compared to deep fixed baselines}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Result Analysis}{22}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion \& Future Work}{23}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Conclusion}{23}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Limitations}{23}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Future Works}{24}{subsection.5.3}\protected@file@percent }
\bibstyle{elsarticle-num}
\bibdata{manuscript_ref}
\bibcite{b11}{{1}{}{{}}{{}}}
\bibcite{b1}{{2}{}{{}}{{}}}
\bibcite{b2}{{3}{}{{}}{{}}}
\bibcite{b3}{{4}{}{{}}{{}}}
\bibcite{b4}{{5}{}{{}}{{}}}
\bibcite{b5}{{6}{}{{}}{{}}}
\bibcite{b6}{{7}{}{{}}{{}}}
\bibcite{b7}{{8}{}{{}}{{}}}
\bibcite{b8}{{9}{}{{}}{{}}}
\bibcite{b9}{{10}{}{{}}{{}}}
\bibcite{b10}{{11}{}{{}}{{}}}
\bibcite{b12}{{12}{}{{}}{{}}}
\bibcite{b13}{{13}{}{{}}{{}}}
\bibcite{b14}{{14}{}{{}}{{}}}
\bibcite{b15}{{15}{}{{}}{{}}}
\bibcite{b16}{{16}{}{{}}{{}}}
\bibcite{b17}{{17}{}{{}}{{}}}
\bibcite{b18}{{18}{}{{}}{{}}}
\bibcite{b19}{{19}{}{{}}{{}}}
\bibcite{b20}{{20}{}{{}}{{}}}
\bibcite{b21}{{21}{}{{}}{{}}}
\bibcite{b22}{{22}{}{{}}{{}}}
\bibcite{b23}{{23}{}{{}}{{}}}
\bibcite{b24}{{24}{}{{}}{{}}}
\bibcite{b25}{{25}{}{{}}{{}}}
\bibcite{b26}{{26}{}{{}}{{}}}
\bibcite{b27}{{27}{}{{}}{{}}}
\bibcite{b28}{{28}{}{{}}{{}}}
\bibcite{b29}{{29}{}{{}}{{}}}
\bibcite{b30}{{30}{}{{}}{{}}}
\bibcite{b31}{{31}{}{{}}{{}}}
\bibcite{b32}{{32}{}{{}}{{}}}
\bibcite{b33}{{33}{}{{}}{{}}}
\bibcite{b34}{{34}{}{{}}{{}}}
\bibcite{b35}{{35}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{28}
