\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{b11}
S.~Bae, Y.~Kim, R.~Bayat, S.~Kim, J.~Ha, T.~Schuster, A.~Fisch, H.~Harutyunyan, Z.~Ji, A.~Courville, S.-Y. Yun, \href{https://arxiv.org/abs/2507.10524}{Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level computation}, arXiv preprint arXiv:2507.10524Accepted to NeurIPS 2025 (2025).
\newline\urlprefix\url{https://arxiv.org/abs/2507.10524}

\bibitem{b1}
J.~Devlin, M.-W. Chang, K.~Lee, K.~Toutanova, {BERT}: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805 (2018).

\bibitem{b2}
T.~Brown, B.~Mann, N.~Ryder, et~al., Language models are few-shot learners, in: Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), Vol.~33, 2020, pp. 1877--1901.

\bibitem{b3}
C.~Raffel, N.~Shazeer, A.~Roberts, et~al., Exploring the limits of transfer learning with a unified text-to-text transformer, J. Mach. Learn. Res. 21~(1) (2020) 5485--5551.

\bibitem{b4}
Z.~Dai, Z.~Yang, Y.~Yang, J.~Carbonell, Q.~V. Le, R.~Salakhutdinov, Transformer-{XL}: Attentive language models beyond a fixed-length context, in: Proc. 57th Annu. Meet. Assoc. Comput. Linguist. (ACL), 2019, pp. 2978--2988.

\bibitem{b5}
N.~Kitaev, L.~Kaiser, A.~Levskaya, Reformer: The efficient transformer, in: Proc. Int. Conf. Learn. Represent. (ICLR), 2020.

\bibitem{b6}
J.~W. Rae, A.~Potapenko, S.~M. Jayakumar, C.~Hillier, T.~P. Lillicrap, Compressive transformers for long-range sequence modelling, arXiv preprint arXiv:1911.05507 (2019).

\bibitem{b7}
L.~B. Allal, et~al., {SmolLM2}: When smol goes big--data-centric training of a small language model, arXiv preprint (2025).

\bibitem{b8}
Y.~Li, Y.~Huang, M.~E. Ildiz, A.~S. Rawat, S.~Oymak, Mechanics of next token prediction with self-attention, arXiv preprint (2024).

\bibitem{b9}
M.~E. Sander, G.~Peyr{\'e}, Towards understanding the universality of transformers for next-token prediction, arXiv preprint arXiv:2410.03011 (2024).

\bibitem{b10}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, J.~Dean, Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, in: Proc. Int. Conf. Learn. Represent. (ICLR), 2017.

\bibitem{b12}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~Kaiser, I.~Polosukhin, Attention is all you need, in: Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2017, pp. 5998--6008.

\bibitem{b13}
H.~Naveed, et~al., A comprehensive overview of large language models, arXiv preprint (2023).

\bibitem{b14}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever, Improving language understanding by generative pre-training, Tech. rep., OpenAI (2018).

\bibitem{b15}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child, S.~Gray, A.~Radford, J.~Wu, D.~Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361 (2020).

\bibitem{b16}
A.~Chowdhery, et~al., {PaLM}: Scaling language modeling with pathways, J. Mach. Learn. Res. 24~(240) (2023) 1--113.

\bibitem{b17}
OpenAI, {GPT}-4 technical report, arXiv preprint arXiv:2303.08774 (2023).

\bibitem{b18}
J.~Okah, Small language models ({SLM}): A comprehensive overview, Hugging Face (2025).

\bibitem{b19}
Q.~Zhang, et~al., The rise of small language models, IEEE Intell. Syst. 40~(1) (2025) 30--37.

\bibitem{b20}
Z.~Lu, et~al., Small language models: Survey, measurements, and insights, arXiv preprint (2025).

\bibitem{b21}
V.~Sanh, L.~Debut, J.~Chaumond, T.~Wolf, {DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter, arXiv preprint arXiv:1910.01108 (2019).

\bibitem{b22}
S.~Gunasekar, Y.~Zhang, J.~Aneja, C.~C.~T. Mendes, et~al., Textbooks are all you need, arXiv preprint arXiv:2306.11644 (2023).

\bibitem{b23}
M.~Javaheripi, et~al., Phi-2: The surprising power of small language models, Tech. rep., Microsoft (2023).

\bibitem{b24}
M.~Abdin, et~al., Phi-3 technical report: A highly capable language model locally on your phone, arXiv preprint arXiv:2404.14219 (2024).

\bibitem{b25}
S.~Bae, A.~Fisch, H.~Harutyunyan, Z.~Ji, S.~Kim, T.~Schuster, Relaxed recursive transformers: Effective parameter sharing with layer-wise {LoRA}, in: Proc. 13th Int. Conf. Learn. Represent. (ICLR), 2025.

\bibitem{b26}
M.~Dehghani, S.~Gouws, O.~Vinyals, J.~Uszkoreit, L.~Kaiser, Universal transformers, in: Proc. Int. Conf. Learn. Represent. (ICLR), 2019.

\bibitem{b27}
W.~Cai, et~al., A survey on mixture of experts in large language models, IEEE Trans. Knowl. Data Eng. 36~(12) (2024) 7304--7317.

\bibitem{b28}
Y.~Zhou, et~al., Mixture-of-experts with expert choice routing, in: Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), Vol.~35, 2022, pp. 7103--7114.

\bibitem{b29}
R.~A. Jacobs, M.~I. Jordan, S.~J. Nowlan, G.~E. Hinton, Adaptive mixtures of local experts, Neural Comput. 3~(1) (1991) 79--87.

\bibitem{b30}
W.~Fedus, B.~Zoph, N.~Shazeer, Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, J. Mach. Learn. Res. 23~(1) (2022) 5232--5270.

\bibitem{b31}
A.~Q. Jiang, et~al., Mixtral of experts, arXiv preprint arXiv:2401.04088 (2024).

\bibitem{b32}
DeepSeek-AI, {DeepSeek}-v2: A strong, economical, and efficient mixture-of-experts language model, arXiv preprint arXiv:2405.04434 (2024).

\bibitem{b33}
S.~Shen, et~al., Sliced recursive transformer, arXiv preprint (2022).

\bibitem{b34}
M.~Elbayad, J.~Gu, E.~Grave, M.~Auli, Depth-adaptive transformer, in: Proc. Int. Conf. Learn. Represent. (ICLR), 2020.

\bibitem{b35}
A.~Karpathy, char-rnn, GitHub repository, \url{https://github.com/karpathy/char-rnn} (2015).

\end{thebibliography}
